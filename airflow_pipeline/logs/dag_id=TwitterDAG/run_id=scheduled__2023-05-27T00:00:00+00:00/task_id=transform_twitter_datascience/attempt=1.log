[2023-05-30 15:06:47,613] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-27T00:00:00+00:00 [queued]>
[2023-05-30 15:06:47,618] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-27T00:00:00+00:00 [queued]>
[2023-05-30 15:06:47,619] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:06:47,619] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-30 15:06:47,619] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:06:47,643] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-05-27 00:00:00+00:00
[2023-05-30 15:06:47,645] {standard_task_runner.py:52} INFO - Started process 15282 to run task
[2023-05-30 15:06:47,648] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-05-27T00:00:00+00:00', '--job-id', '30', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp1r2e7rni', '--error-file', '/tmp/tmpb2xrr7ic']
[2023-05-30 15:06:47,648] {standard_task_runner.py:80} INFO - Job 30: Subtask transform_twitter_datascience
[2023-05-30 15:06:47,696] {task_command.py:370} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-27T00:00:00+00:00 [running]> on host BIRIBA.localdomain
[2023-05-30 15:06:47,759] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=TwitterDAG
AIRFLOW_CTX_TASK_ID=transform_twitter_datascience
AIRFLOW_CTX_EXECUTION_DATE=2023-05-27T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-05-27T00:00:00+00:00
[2023-05-30 15:06:47,763] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2023-05-30 15:06:47,764] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/aluno/Documents/curso2/src/spark/transformation.py --src curso2/datalake/twitter_datascience --dest curso2/Silver/twitter_datascience --process-date 2023-05-27
[2023-05-30 15:06:49,704] {spark_submit.py:495} INFO - 23/05/30 15:06:49 WARN Utils: Your hostname, BIRIBA resolves to a loopback address: 127.0.1.1; using 172.19.51.55 instead (on interface eth0)
[2023-05-30 15:06:49,709] {spark_submit.py:495} INFO - 23/05/30 15:06:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-05-30 15:06:51,294] {spark_submit.py:495} INFO - python3: can't open file '/home/aluno/Documents/curso2/src/spark/transformation.py': [Errno 2] No such file or directory
[2023-05-30 15:06:51,300] {spark_submit.py:495} INFO - 23/05/30 15:06:51 INFO ShutdownHookManager: Shutdown hook called
[2023-05-30 15:06:51,301] {spark_submit.py:495} INFO - 23/05/30 15:06:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-c4717951-61ac-45f7-a2b2-33410da0fa61
[2023-05-30 15:06:51,338] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/raf/pipeline/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/raf/pipeline/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name twitter_transformation --queue root.default /home/aluno/Documents/curso2/src/spark/transformation.py --src curso2/datalake/twitter_datascience --dest curso2/Silver/twitter_datascience --process-date 2023-05-27. Error code is: 2.
[2023-05-30 15:06:51,341] {taskinstance.py:1395} INFO - Marking task as FAILED. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230527T000000, start_date=20230530T180647, end_date=20230530T180651
[2023-05-30 15:06:51,359] {standard_task_runner.py:92} ERROR - Failed to execute job 30 for task transform_twitter_datascience (Cannot execute: spark-submit --master local --name twitter_transformation --queue root.default /home/aluno/Documents/curso2/src/spark/transformation.py --src curso2/datalake/twitter_datascience --dest curso2/Silver/twitter_datascience --process-date 2023-05-27. Error code is: 2.; 15282)
[2023-05-30 15:06:51,401] {local_task_job.py:156} INFO - Task exited with return code 1
[2023-05-30 15:06:51,416] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-30 15:12:23,518] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-27T00:00:00+00:00 [queued]>
[2023-05-30 15:12:23,531] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-27T00:00:00+00:00 [queued]>
[2023-05-30 15:12:23,531] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:12:23,531] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-30 15:12:23,531] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:12:23,556] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-05-27 00:00:00+00:00
[2023-05-30 15:12:23,560] {standard_task_runner.py:52} INFO - Started process 18312 to run task
[2023-05-30 15:12:23,564] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-05-27T00:00:00+00:00', '--job-id', '30', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp2waouplh', '--error-file', '/tmp/tmp5nam2qu7']
[2023-05-30 15:12:23,564] {standard_task_runner.py:80} INFO - Job 30: Subtask transform_twitter_datascience
[2023-05-30 15:12:23,619] {task_command.py:370} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-27T00:00:00+00:00 [running]> on host BIRIBA.localdomain
[2023-05-30 15:12:23,679] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=TwitterDAG
AIRFLOW_CTX_TASK_ID=transform_twitter_datascience
AIRFLOW_CTX_EXECUTION_DATE=2023-05-27T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-05-27T00:00:00+00:00
[2023-05-30 15:12:23,683] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2023-05-30 15:12:23,684] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/raf/pipeline-transform/src/spark/transformation.py --src curso2/datalake/twitter_datascience --dest curso2/Silver/twitter_datascience --process-date 2023-05-27
[2023-05-30 15:12:26,047] {spark_submit.py:495} INFO - 23/05/30 15:12:26 WARN Utils: Your hostname, BIRIBA resolves to a loopback address: 127.0.1.1; using 172.19.51.55 instead (on interface eth0)
[2023-05-30 15:12:26,051] {spark_submit.py:495} INFO - 23/05/30 15:12:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-05-30 15:12:28,591] {spark_submit.py:495} INFO - 23/05/30 15:12:28 INFO SparkContext: Running Spark version 3.3.1
[2023-05-30 15:12:28,760] {spark_submit.py:495} INFO - 23/05/30 15:12:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-05-30 15:12:28,926] {spark_submit.py:495} INFO - 23/05/30 15:12:28 INFO ResourceUtils: ==============================================================
[2023-05-30 15:12:28,927] {spark_submit.py:495} INFO - 23/05/30 15:12:28 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-05-30 15:12:28,927] {spark_submit.py:495} INFO - 23/05/30 15:12:28 INFO ResourceUtils: ==============================================================
[2023-05-30 15:12:28,928] {spark_submit.py:495} INFO - 23/05/30 15:12:28 INFO SparkContext: Submitted application: twitter_transformation
[2023-05-30 15:12:28,968] {spark_submit.py:495} INFO - 23/05/30 15:12:28 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-05-30 15:12:28,985] {spark_submit.py:495} INFO - 23/05/30 15:12:28 INFO ResourceProfile: Limiting resource is cpu
[2023-05-30 15:12:28,986] {spark_submit.py:495} INFO - 23/05/30 15:12:28 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-05-30 15:12:29,066] {spark_submit.py:495} INFO - 23/05/30 15:12:29 INFO SecurityManager: Changing view acls to: raf
[2023-05-30 15:12:29,067] {spark_submit.py:495} INFO - 23/05/30 15:12:29 INFO SecurityManager: Changing modify acls to: raf
[2023-05-30 15:12:29,068] {spark_submit.py:495} INFO - 23/05/30 15:12:29 INFO SecurityManager: Changing view acls groups to:
[2023-05-30 15:12:29,069] {spark_submit.py:495} INFO - 23/05/30 15:12:29 INFO SecurityManager: Changing modify acls groups to:
[2023-05-30 15:12:29,070] {spark_submit.py:495} INFO - 23/05/30 15:12:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(raf); groups with view permissions: Set(); users  with modify permissions: Set(raf); groups with modify permissions: Set()
[2023-05-30 15:12:29,464] {spark_submit.py:495} INFO - 23/05/30 15:12:29 INFO Utils: Successfully started service 'sparkDriver' on port 43167.
[2023-05-30 15:12:29,497] {spark_submit.py:495} INFO - 23/05/30 15:12:29 INFO SparkEnv: Registering MapOutputTracker
[2023-05-30 15:12:29,548] {spark_submit.py:495} INFO - 23/05/30 15:12:29 INFO SparkEnv: Registering BlockManagerMaster
[2023-05-30 15:12:29,597] {spark_submit.py:495} INFO - 23/05/30 15:12:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-05-30 15:12:29,597] {spark_submit.py:495} INFO - 23/05/30 15:12:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-05-30 15:12:29,608] {spark_submit.py:495} INFO - 23/05/30 15:12:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-05-30 15:12:29,710] {spark_submit.py:495} INFO - 23/05/30 15:12:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e7548ef6-b0a4-4e22-b881-e0b3d918217c
[2023-05-30 15:12:29,741] {spark_submit.py:495} INFO - 23/05/30 15:12:29 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2023-05-30 15:12:29,777] {spark_submit.py:495} INFO - 23/05/30 15:12:29 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-05-30 15:12:30,103] {spark_submit.py:495} INFO - 23/05/30 15:12:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-05-30 15:12:30,286] {spark_submit.py:495} INFO - 23/05/30 15:12:30 INFO Executor: Starting executor ID driver on host 172.19.51.55
[2023-05-30 15:12:30,299] {spark_submit.py:495} INFO - 23/05/30 15:12:30 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-05-30 15:12:30,330] {spark_submit.py:495} INFO - 23/05/30 15:12:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41555.
[2023-05-30 15:12:30,330] {spark_submit.py:495} INFO - 23/05/30 15:12:30 INFO NettyBlockTransferService: Server created on 172.19.51.55:41555
[2023-05-30 15:12:30,332] {spark_submit.py:495} INFO - 23/05/30 15:12:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-05-30 15:12:30,339] {spark_submit.py:495} INFO - 23/05/30 15:12:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.19.51.55, 41555, None)
[2023-05-30 15:12:30,344] {spark_submit.py:495} INFO - 23/05/30 15:12:30 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.51.55:41555 with 366.3 MiB RAM, BlockManagerId(driver, 172.19.51.55, 41555, None)
[2023-05-30 15:12:30,351] {spark_submit.py:495} INFO - 23/05/30 15:12:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.19.51.55, 41555, None)
[2023-05-30 15:12:30,352] {spark_submit.py:495} INFO - 23/05/30 15:12:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.19.51.55, 41555, None)
[2023-05-30 15:12:31,023] {spark_submit.py:495} INFO - 23/05/30 15:12:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-05-30 15:12:31,032] {spark_submit.py:495} INFO - 23/05/30 15:12:31 INFO SharedState: Warehouse path is 'file:/home/raf/pipeline-transform/airflow_pipeline/spark-warehouse'.
[2023-05-30 15:12:32,316] {spark_submit.py:495} INFO - 23/05/30 15:12:32 INFO InMemoryFileIndex: It took 83 ms to list leaf files for 1 paths.
[2023-05-30 15:12:32,501] {spark_submit.py:495} INFO - 23/05/30 15:12:32 INFO InMemoryFileIndex: It took 10 ms to list leaf files for 6 paths.
[2023-05-30 15:12:35,407] {spark_submit.py:495} INFO - 23/05/30 15:12:35 INFO FileSourceStrategy: Pushed Filters:
[2023-05-30 15:12:35,410] {spark_submit.py:495} INFO - 23/05/30 15:12:35 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-30 15:12:35,414] {spark_submit.py:495} INFO - 23/05/30 15:12:35 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-05-30 15:12:35,794] {spark_submit.py:495} INFO - 23/05/30 15:12:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 349.6 KiB, free 366.0 MiB)
[2023-05-30 15:12:35,868] {spark_submit.py:495} INFO - 23/05/30 15:12:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 365.9 MiB)
[2023-05-30 15:12:35,871] {spark_submit.py:495} INFO - 23/05/30 15:12:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.51.55:41555 (size: 33.9 KiB, free: 366.3 MiB)
[2023-05-30 15:12:35,877] {spark_submit.py:495} INFO - 23/05/30 15:12:35 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:12:36,055] {spark_submit.py:495} INFO - 23/05/30 15:12:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25192964 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:12:36,327] {spark_submit.py:495} INFO - 23/05/30 15:12:36 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:12:36,355] {spark_submit.py:495} INFO - 23/05/30 15:12:36 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:12:36,356] {spark_submit.py:495} INFO - 23/05/30 15:12:36 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:12:36,357] {spark_submit.py:495} INFO - 23/05/30 15:12:36 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:12:36,362] {spark_submit.py:495} INFO - 23/05/30 15:12:36 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:12:36,368] {spark_submit.py:495} INFO - 23/05/30 15:12:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:12:36,539] {spark_submit.py:495} INFO - 23/05/30 15:12:36 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 365.9 MiB)
[2023-05-30 15:12:36,545] {spark_submit.py:495} INFO - 23/05/30 15:12:36 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 365.9 MiB)
[2023-05-30 15:12:36,548] {spark_submit.py:495} INFO - 23/05/30 15:12:36 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.51.55:41555 (size: 7.0 KiB, free: 366.3 MiB)
[2023-05-30 15:12:36,551] {spark_submit.py:495} INFO - 23/05/30 15:12:36 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:12:36,573] {spark_submit.py:495} INFO - 23/05/30 15:12:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:12:36,574] {spark_submit.py:495} INFO - 23/05/30 15:12:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-05-30 15:12:36,645] {spark_submit.py:495} INFO - 23/05/30 15:12:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 5929 bytes) taskResourceAssignments Map()
[2023-05-30 15:12:36,664] {spark_submit.py:495} INFO - 23/05/30 15:12:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-05-30 15:12:37,076] {spark_submit.py:495} INFO - 23/05/30 15:12:37 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-28/datascience_20230528.json, range: 0-4545, partition values: [empty row]
[2023-05-30 15:12:37,362] {spark_submit.py:495} INFO - 23/05/30 15:12:37 INFO CodeGenerator: Code generated in 216.8227 ms
[2023-05-30 15:12:37,418] {spark_submit.py:495} INFO - 23/05/30 15:12:37 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-29/datascience_20230529.json, range: 0-4545, partition values: [empty row]
[2023-05-30 15:12:37,426] {spark_submit.py:495} INFO - 23/05/30 15:12:37 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-27/datascience_20230527.json, range: 0-4542, partition values: [empty row]
[2023-05-30 15:12:37,433] {spark_submit.py:495} INFO - 23/05/30 15:12:37 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-24/datascience_20230524.json, range: 0-4518, partition values: [empty row]
[2023-05-30 15:12:37,437] {spark_submit.py:495} INFO - 23/05/30 15:12:37 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-26/datascience_20230526.json, range: 0-4506, partition values: [empty row]
[2023-05-30 15:12:37,441] {spark_submit.py:495} INFO - 23/05/30 15:12:37 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-4484, partition values: [empty row]
[2023-05-30 15:12:37,470] {spark_submit.py:495} INFO - 23/05/30 15:12:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
[2023-05-30 15:12:37,483] {spark_submit.py:495} INFO - 23/05/30 15:12:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 849 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:12:37,485] {spark_submit.py:495} INFO - 23/05/30 15:12:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-05-30 15:12:37,491] {spark_submit.py:495} INFO - 23/05/30 15:12:37 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.099 s
[2023-05-30 15:12:37,499] {spark_submit.py:495} INFO - 23/05/30 15:12:37 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:12:37,500] {spark_submit.py:495} INFO - 23/05/30 15:12:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-05-30 15:12:37,504] {spark_submit.py:495} INFO - 23/05/30 15:12:37 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.175083 s
[2023-05-30 15:12:38,168] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO DataSourceStrategy: Pruning directories with:
[2023-05-30 15:12:38,172] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-05-30 15:12:38,174] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-05-30 15:12:38,175] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-05-30 15:12:38,293] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:12:38,295] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:12:38,296] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:12:38,609] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO CodeGenerator: Code generated in 187.5919 ms
[2023-05-30 15:12:38,620] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 349.5 KiB, free 365.6 MiB)
[2023-05-30 15:12:38,636] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.5 MiB)
[2023-05-30 15:12:38,638] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.51.55:41555 (size: 33.8 KiB, free: 366.2 MiB)
[2023-05-30 15:12:38,640] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:12:38,649] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25192964 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:12:38,756] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:12:38,761] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:12:38,762] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:12:38,762] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:12:38,762] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:12:38,774] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:12:38,823] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.2 KiB, free 365.3 MiB)
[2023-05-30 15:12:38,831] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.2 KiB, free 365.2 MiB)
[2023-05-30 15:12:38,833] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.19.51.55:41555 (size: 83.2 KiB, free: 366.1 MiB)
[2023-05-30 15:12:38,835] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:12:38,837] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:12:38,838] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-05-30 15:12:38,845] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 6273 bytes) taskResourceAssignments Map()
[2023-05-30 15:12:38,847] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-05-30 15:12:38,935] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:12:38,936] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:12:38,936] {spark_submit.py:495} INFO - 23/05/30 15:12:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:12:39,002] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-28/datascience_20230528.json, range: 0-4545, partition values: [19505]
[2023-05-30 15:12:39,047] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO CodeGenerator: Code generated in 39.9256 ms
[2023-05-30 15:12:39,092] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO CodeGenerator: Code generated in 10.2372 ms
[2023-05-30 15:12:39,124] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-29/datascience_20230529.json, range: 0-4545, partition values: [19506]
[2023-05-30 15:12:39,133] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-27/datascience_20230527.json, range: 0-4542, partition values: [19504]
[2023-05-30 15:12:39,143] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-24/datascience_20230524.json, range: 0-4518, partition values: [19501]
[2023-05-30 15:12:39,152] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-26/datascience_20230526.json, range: 0-4506, partition values: [19503]
[2023-05-30 15:12:39,161] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-4484, partition values: [19502]
[2023-05-30 15:12:39,182] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileOutputCommitter: Saved output of task 'attempt_202305301512381853056407846130891_0001_m_000000_1' to file:/home/raf/pipeline-transform/airflow_pipeline/curso2/Silver/twitter_datascience/tweet/process_date=2023-05-27/_temporary/0/task_202305301512381853056407846130891_0001_m_000000
[2023-05-30 15:12:39,183] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO SparkHadoopMapRedUtil: attempt_202305301512381853056407846130891_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-05-30 15:12:39,190] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-05-30 15:12:39,195] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 353 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:12:39,195] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-05-30 15:12:39,197] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.431 s
[2023-05-30 15:12:39,197] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:12:39,197] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-05-30 15:12:39,198] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0.441405 s
[2023-05-30 15:12:39,201] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileFormatWriter: Start to commit write Job b9c0741a-8271-490e-b5ba-b64ea282e331.
[2023-05-30 15:12:39,222] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileFormatWriter: Write Job b9c0741a-8271-490e-b5ba-b64ea282e331 committed. Elapsed time: 20 ms.
[2023-05-30 15:12:39,225] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileFormatWriter: Finished processing stats for write job b9c0741a-8271-490e-b5ba-b64ea282e331.
[2023-05-30 15:12:39,283] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO DataSourceStrategy: Pruning directories with:
[2023-05-30 15:12:39,284] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileSourceStrategy: Pushed Filters:
[2023-05-30 15:12:39,284] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-30 15:12:39,285] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-05-30 15:12:39,298] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:12:39,298] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:12:39,298] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:12:39,364] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.19.51.55:41555 in memory (size: 33.8 KiB, free: 366.2 MiB)
[2023-05-30 15:12:39,373] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.19.51.55:41555 in memory (size: 83.2 KiB, free: 366.3 MiB)
[2023-05-30 15:12:39,388] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.19.51.55:41555 in memory (size: 7.0 KiB, free: 366.3 MiB)
[2023-05-30 15:12:39,397] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO CodeGenerator: Code generated in 45.4402 ms
[2023-05-30 15:12:39,402] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 349.5 KiB, free 365.6 MiB)
[2023-05-30 15:12:39,413] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.6 MiB)
[2023-05-30 15:12:39,414] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.19.51.55:41555 (size: 33.8 KiB, free: 366.2 MiB)
[2023-05-30 15:12:39,416] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:12:39,418] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25192964 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:12:39,455] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:12:39,457] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:12:39,457] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:12:39,457] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:12:39,458] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:12:39,460] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:12:39,489] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.2 KiB, free 365.3 MiB)
[2023-05-30 15:12:39,492] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.5 KiB, free 365.3 MiB)
[2023-05-30 15:12:39,494] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.19.51.55:41555 (size: 76.5 KiB, free: 366.2 MiB)
[2023-05-30 15:12:39,495] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:12:39,497] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:12:39,497] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-05-30 15:12:39,500] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 6273 bytes) taskResourceAssignments Map()
[2023-05-30 15:12:39,501] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-05-30 15:12:39,531] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:12:39,532] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:12:39,533] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:12:39,572] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-28/datascience_20230528.json, range: 0-4545, partition values: [19505]
[2023-05-30 15:12:39,620] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO CodeGenerator: Code generated in 39.7629 ms
[2023-05-30 15:12:39,630] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-29/datascience_20230529.json, range: 0-4545, partition values: [19506]
[2023-05-30 15:12:39,635] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-27/datascience_20230527.json, range: 0-4542, partition values: [19504]
[2023-05-30 15:12:39,641] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-24/datascience_20230524.json, range: 0-4518, partition values: [19501]
[2023-05-30 15:12:39,648] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-26/datascience_20230526.json, range: 0-4506, partition values: [19503]
[2023-05-30 15:12:39,652] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-4484, partition values: [19502]
[2023-05-30 15:12:39,660] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileOutputCommitter: Saved output of task 'attempt_202305301512398580428892106206798_0002_m_000000_2' to file:/home/raf/pipeline-transform/airflow_pipeline/curso2/Silver/twitter_datascience/user/process_date=2023-05-27/_temporary/0/task_202305301512398580428892106206798_0002_m_000000
[2023-05-30 15:12:39,661] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO SparkHadoopMapRedUtil: attempt_202305301512398580428892106206798_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-05-30 15:12:39,664] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-05-30 15:12:39,666] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 166 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:12:39,668] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.206 s
[2023-05-30 15:12:39,668] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:12:39,668] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-05-30 15:12:39,668] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-05-30 15:12:39,668] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.212622 s
[2023-05-30 15:12:39,669] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileFormatWriter: Start to commit write Job cea03a46-4523-416a-b028-43d675086855.
[2023-05-30 15:12:39,692] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileFormatWriter: Write Job cea03a46-4523-416a-b028-43d675086855 committed. Elapsed time: 23 ms.
[2023-05-30 15:12:39,693] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO FileFormatWriter: Finished processing stats for write job cea03a46-4523-416a-b028-43d675086855.
[2023-05-30 15:12:39,729] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO SparkContext: Invoking stop() from shutdown hook
[2023-05-30 15:12:39,741] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO SparkUI: Stopped Spark web UI at http://172.19.51.55:4040
[2023-05-30 15:12:39,765] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-05-30 15:12:39,776] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO MemoryStore: MemoryStore cleared
[2023-05-30 15:12:39,778] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO BlockManager: BlockManager stopped
[2023-05-30 15:12:39,783] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-05-30 15:12:39,785] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-05-30 15:12:39,789] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO SparkContext: Successfully stopped SparkContext
[2023-05-30 15:12:39,790] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO ShutdownHookManager: Shutdown hook called
[2023-05-30 15:12:39,790] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-6b4518e3-3560-4f47-9c13-ee1d97cc6267
[2023-05-30 15:12:39,793] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-2a200b88-3db2-4db5-8513-44aa1ae7a9ab
[2023-05-30 15:12:39,796] {spark_submit.py:495} INFO - 23/05/30 15:12:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-2a200b88-3db2-4db5-8513-44aa1ae7a9ab/pyspark-9f6c00bc-f9c6-4aa4-b42b-79707b11f023
[2023-05-30 15:12:39,837] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230527T000000, start_date=20230530T181223, end_date=20230530T181239
[2023-05-30 15:12:39,882] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-30 15:12:39,895] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-30 15:24:07,579] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-27T00:00:00+00:00 [queued]>
[2023-05-30 15:24:07,587] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-27T00:00:00+00:00 [queued]>
[2023-05-30 15:24:07,587] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:24:07,587] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-30 15:24:07,587] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:24:07,607] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-05-27 00:00:00+00:00
[2023-05-30 15:24:07,608] {standard_task_runner.py:52} INFO - Started process 21801 to run task
[2023-05-30 15:24:07,611] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-05-27T00:00:00+00:00', '--job-id', '44', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp20_om7bf', '--error-file', '/tmp/tmpsejum6pe']
[2023-05-30 15:24:07,612] {standard_task_runner.py:80} INFO - Job 44: Subtask transform_twitter_datascience
[2023-05-30 15:24:07,661] {task_command.py:370} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-27T00:00:00+00:00 [running]> on host BIRIBA.localdomain
[2023-05-30 15:24:07,723] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=TwitterDAG
AIRFLOW_CTX_TASK_ID=transform_twitter_datascience
AIRFLOW_CTX_EXECUTION_DATE=2023-05-27T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-05-27T00:00:00+00:00
[2023-05-30 15:24:07,732] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2023-05-30 15:24:07,735] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/raf/pipeline-transform/src/spark/transformation.py --src /home/raf/pipeline-transform/datalake/bronze/twitter_datascience --dest /home/raf/pipeline-transform/datalake/silver/twitter_datascience --process-date 2023-05-27
[2023-05-30 15:24:09,783] {spark_submit.py:495} INFO - 23/05/30 15:24:09 WARN Utils: Your hostname, BIRIBA resolves to a loopback address: 127.0.1.1; using 172.19.51.55 instead (on interface eth0)
[2023-05-30 15:24:09,787] {spark_submit.py:495} INFO - 23/05/30 15:24:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-05-30 15:24:11,784] {spark_submit.py:495} INFO - 23/05/30 15:24:11 INFO SparkContext: Running Spark version 3.3.1
[2023-05-30 15:24:11,908] {spark_submit.py:495} INFO - 23/05/30 15:24:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-05-30 15:24:12,015] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO ResourceUtils: ==============================================================
[2023-05-30 15:24:12,016] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-05-30 15:24:12,016] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO ResourceUtils: ==============================================================
[2023-05-30 15:24:12,017] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO SparkContext: Submitted application: twitter_transformation
[2023-05-30 15:24:12,047] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-05-30 15:24:12,060] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO ResourceProfile: Limiting resource is cpu
[2023-05-30 15:24:12,061] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-05-30 15:24:12,126] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO SecurityManager: Changing view acls to: raf
[2023-05-30 15:24:12,127] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO SecurityManager: Changing modify acls to: raf
[2023-05-30 15:24:12,128] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO SecurityManager: Changing view acls groups to:
[2023-05-30 15:24:12,129] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO SecurityManager: Changing modify acls groups to:
[2023-05-30 15:24:12,131] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(raf); groups with view permissions: Set(); users  with modify permissions: Set(raf); groups with modify permissions: Set()
[2023-05-30 15:24:12,463] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO Utils: Successfully started service 'sparkDriver' on port 41625.
[2023-05-30 15:24:12,508] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO SparkEnv: Registering MapOutputTracker
[2023-05-30 15:24:12,563] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO SparkEnv: Registering BlockManagerMaster
[2023-05-30 15:24:12,591] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-05-30 15:24:12,592] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-05-30 15:24:12,599] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-05-30 15:24:12,683] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-26390915-d685-47ec-b7c6-49208f26741d
[2023-05-30 15:24:12,716] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2023-05-30 15:24:12,747] {spark_submit.py:495} INFO - 23/05/30 15:24:12 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-05-30 15:24:13,099] {spark_submit.py:495} INFO - 23/05/30 15:24:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-05-30 15:24:13,279] {spark_submit.py:495} INFO - 23/05/30 15:24:13 INFO Executor: Starting executor ID driver on host 172.19.51.55
[2023-05-30 15:24:13,290] {spark_submit.py:495} INFO - 23/05/30 15:24:13 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-05-30 15:24:13,323] {spark_submit.py:495} INFO - 23/05/30 15:24:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36709.
[2023-05-30 15:24:13,323] {spark_submit.py:495} INFO - 23/05/30 15:24:13 INFO NettyBlockTransferService: Server created on 172.19.51.55:36709
[2023-05-30 15:24:13,326] {spark_submit.py:495} INFO - 23/05/30 15:24:13 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-05-30 15:24:13,333] {spark_submit.py:495} INFO - 23/05/30 15:24:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.19.51.55, 36709, None)
[2023-05-30 15:24:13,338] {spark_submit.py:495} INFO - 23/05/30 15:24:13 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.51.55:36709 with 366.3 MiB RAM, BlockManagerId(driver, 172.19.51.55, 36709, None)
[2023-05-30 15:24:13,343] {spark_submit.py:495} INFO - 23/05/30 15:24:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.19.51.55, 36709, None)
[2023-05-30 15:24:13,345] {spark_submit.py:495} INFO - 23/05/30 15:24:13 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.19.51.55, 36709, None)
[2023-05-30 15:24:13,989] {spark_submit.py:495} INFO - 23/05/30 15:24:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-05-30 15:24:13,997] {spark_submit.py:495} INFO - 23/05/30 15:24:13 INFO SharedState: Warehouse path is 'file:/home/raf/pipeline-transform/airflow_pipeline/spark-warehouse'.
[2023-05-30 15:24:15,341] {spark_submit.py:495} INFO - 23/05/30 15:24:15 INFO InMemoryFileIndex: It took 90 ms to list leaf files for 1 paths.
[2023-05-30 15:24:15,654] {spark_submit.py:495} INFO - 23/05/30 15:24:15 INFO InMemoryFileIndex: It took 17 ms to list leaf files for 5 paths.
[2023-05-30 15:24:18,626] {spark_submit.py:495} INFO - 23/05/30 15:24:18 INFO FileSourceStrategy: Pushed Filters:
[2023-05-30 15:24:18,628] {spark_submit.py:495} INFO - 23/05/30 15:24:18 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-30 15:24:18,632] {spark_submit.py:495} INFO - 23/05/30 15:24:18 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-05-30 15:24:19,017] {spark_submit.py:495} INFO - 23/05/30 15:24:19 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 349.6 KiB, free 366.0 MiB)
[2023-05-30 15:24:19,097] {spark_submit.py:495} INFO - 23/05/30 15:24:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 365.9 MiB)
[2023-05-30 15:24:19,100] {spark_submit.py:495} INFO - 23/05/30 15:24:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.51.55:36709 (size: 33.9 KiB, free: 366.3 MiB)
[2023-05-30 15:24:19,106] {spark_submit.py:495} INFO - 23/05/30 15:24:19 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:24:19,230] {spark_submit.py:495} INFO - 23/05/30 15:24:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 21035047 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:24:19,493] {spark_submit.py:495} INFO - 23/05/30 15:24:19 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:24:19,521] {spark_submit.py:495} INFO - 23/05/30 15:24:19 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:24:19,523] {spark_submit.py:495} INFO - 23/05/30 15:24:19 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:24:19,523] {spark_submit.py:495} INFO - 23/05/30 15:24:19 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:24:19,526] {spark_submit.py:495} INFO - 23/05/30 15:24:19 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:24:19,534] {spark_submit.py:495} INFO - 23/05/30 15:24:19 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:24:19,675] {spark_submit.py:495} INFO - 23/05/30 15:24:19 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 365.9 MiB)
[2023-05-30 15:24:19,679] {spark_submit.py:495} INFO - 23/05/30 15:24:19 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 365.9 MiB)
[2023-05-30 15:24:19,680] {spark_submit.py:495} INFO - 23/05/30 15:24:19 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.51.55:36709 (size: 7.0 KiB, free: 366.3 MiB)
[2023-05-30 15:24:19,682] {spark_submit.py:495} INFO - 23/05/30 15:24:19 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:24:19,702] {spark_submit.py:495} INFO - 23/05/30 15:24:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:24:19,703] {spark_submit.py:495} INFO - 23/05/30 15:24:19 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-05-30 15:24:19,764] {spark_submit.py:495} INFO - 23/05/30 15:24:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 5660 bytes) taskResourceAssignments Map()
[2023-05-30 15:24:19,782] {spark_submit.py:495} INFO - 23/05/30 15:24:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-05-30 15:24:20,198] {spark_submit.py:495} INFO - 23/05/30 15:24:20 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-26/datascience_20230526.json, range: 0-23089, partition values: [empty row]
[2023-05-30 15:24:20,476] {spark_submit.py:495} INFO - 23/05/30 15:24:20 INFO CodeGenerator: Code generated in 198.8399 ms
[2023-05-30 15:24:20,668] {spark_submit.py:495} INFO - 23/05/30 15:24:20 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-27/datascience_20230527.json, range: 0-18079, partition values: [empty row]
[2023-05-30 15:24:20,700] {spark_submit.py:495} INFO - 23/05/30 15:24:20 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-24/datascience_20230524.json, range: 0-9003, partition values: [empty row]
[2023-05-30 15:24:20,716] {spark_submit.py:495} INFO - 23/05/30 15:24:20 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-8881, partition values: [empty row]
[2023-05-30 15:24:20,736] {spark_submit.py:495} INFO - 23/05/30 15:24:20 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-28/datascience_20230528.json, range: 0-4475, partition values: [empty row]
[2023-05-30 15:24:20,824] {spark_submit.py:495} INFO - 23/05/30 15:24:20 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2849 bytes result sent to driver
[2023-05-30 15:24:20,873] {spark_submit.py:495} INFO - 23/05/30 15:24:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1115 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:24:20,883] {spark_submit.py:495} INFO - 23/05/30 15:24:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-05-30 15:24:20,894] {spark_submit.py:495} INFO - 23/05/30 15:24:20 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.335 s
[2023-05-30 15:24:20,900] {spark_submit.py:495} INFO - 23/05/30 15:24:20 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:24:20,902] {spark_submit.py:495} INFO - 23/05/30 15:24:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-05-30 15:24:20,910] {spark_submit.py:495} INFO - 23/05/30 15:24:20 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.415345 s
[2023-05-30 15:24:21,727] {spark_submit.py:495} INFO - 23/05/30 15:24:21 INFO DataSourceStrategy: Pruning directories with:
[2023-05-30 15:24:21,732] {spark_submit.py:495} INFO - 23/05/30 15:24:21 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-05-30 15:24:21,735] {spark_submit.py:495} INFO - 23/05/30 15:24:21 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-05-30 15:24:21,736] {spark_submit.py:495} INFO - 23/05/30 15:24:21 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-05-30 15:24:21,857] {spark_submit.py:495} INFO - 23/05/30 15:24:21 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:24:21,857] {spark_submit.py:495} INFO - 23/05/30 15:24:21 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:24:21,859] {spark_submit.py:495} INFO - 23/05/30 15:24:21 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:24:22,286] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO CodeGenerator: Code generated in 293.9344 ms
[2023-05-30 15:24:22,296] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 349.5 KiB, free 365.6 MiB)
[2023-05-30 15:24:22,313] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.5 MiB)
[2023-05-30 15:24:22,315] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.51.55:36709 (size: 33.8 KiB, free: 366.2 MiB)
[2023-05-30 15:24:22,316] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:24:22,325] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 21035047 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:24:22,498] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:24:22,500] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:24:22,501] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:24:22,501] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:24:22,501] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:24:22,502] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:24:22,602] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.1 KiB, free 365.3 MiB)
[2023-05-30 15:24:22,610] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.3 KiB, free 365.2 MiB)
[2023-05-30 15:24:22,611] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.19.51.55:36709 (size: 83.3 KiB, free: 366.1 MiB)
[2023-05-30 15:24:22,612] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:24:22,613] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:24:22,614] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-05-30 15:24:22,620] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 5983 bytes) taskResourceAssignments Map()
[2023-05-30 15:24:22,625] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-05-30 15:24:22,709] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:24:22,710] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:24:22,711] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:24:22,767] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-26/datascience_20230526.json, range: 0-23089, partition values: [19503]
[2023-05-30 15:24:22,813] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO CodeGenerator: Code generated in 39.0099 ms
[2023-05-30 15:24:22,854] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO CodeGenerator: Code generated in 8.5855 ms
[2023-05-30 15:24:22,900] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-27/datascience_20230527.json, range: 0-18079, partition values: [19504]
[2023-05-30 15:24:22,921] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-24/datascience_20230524.json, range: 0-9003, partition values: [19501]
[2023-05-30 15:24:22,957] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-8881, partition values: [19502]
[2023-05-30 15:24:22,965] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-28/datascience_20230528.json, range: 0-4475, partition values: [19505]
[2023-05-30 15:24:22,986] {spark_submit.py:495} INFO - 23/05/30 15:24:22 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.19.51.55:36709 in memory (size: 7.0 KiB, free: 366.2 MiB)
[2023-05-30 15:24:23,002] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileOutputCommitter: Saved output of task 'attempt_202305301524226401382420954377971_0001_m_000000_1' to file:/home/raf/pipeline-transform/datalake/silver/twitter_datascience/tweet/process_date=2023-05-27/_temporary/0/task_202305301524226401382420954377971_0001_m_000000
[2023-05-30 15:24:23,003] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO SparkHadoopMapRedUtil: attempt_202305301524226401382420954377971_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-05-30 15:24:23,016] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2721 bytes result sent to driver
[2023-05-30 15:24:23,020] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 405 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:24:23,021] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-05-30 15:24:23,022] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.518 s
[2023-05-30 15:24:23,024] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:24:23,024] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-05-30 15:24:23,027] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0.528949 s
[2023-05-30 15:24:23,034] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileFormatWriter: Start to commit write Job d3bac98f-c252-462e-9f9d-02603733b28b.
[2023-05-30 15:24:23,058] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileFormatWriter: Write Job d3bac98f-c252-462e-9f9d-02603733b28b committed. Elapsed time: 21 ms.
[2023-05-30 15:24:23,064] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileFormatWriter: Finished processing stats for write job d3bac98f-c252-462e-9f9d-02603733b28b.
[2023-05-30 15:24:23,136] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO DataSourceStrategy: Pruning directories with:
[2023-05-30 15:24:23,137] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileSourceStrategy: Pushed Filters:
[2023-05-30 15:24:23,138] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-30 15:24:23,141] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-05-30 15:24:23,155] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:24:23,157] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:24:23,160] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:24:23,223] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO CodeGenerator: Code generated in 34.7732 ms
[2023-05-30 15:24:23,233] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 349.5 KiB, free 364.9 MiB)
[2023-05-30 15:24:23,292] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 364.9 MiB)
[2023-05-30 15:24:23,293] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.19.51.55:36709 (size: 33.8 KiB, free: 366.1 MiB)
[2023-05-30 15:24:23,294] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:24:23,296] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 21035047 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:24:23,321] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:24:23,323] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:24:23,323] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:24:23,323] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:24:23,324] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:24:23,326] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:24:23,344] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.1 KiB, free 364.7 MiB)
[2023-05-30 15:24:23,346] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.5 KiB, free 364.6 MiB)
[2023-05-30 15:24:23,348] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.19.51.55:36709 (size: 76.5 KiB, free: 366.0 MiB)
[2023-05-30 15:24:23,348] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:24:23,349] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:24:23,349] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-05-30 15:24:23,350] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 5983 bytes) taskResourceAssignments Map()
[2023-05-30 15:24:23,351] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-05-30 15:24:23,376] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:24:23,376] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:24:23,377] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:24:23,401] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-26/datascience_20230526.json, range: 0-23089, partition values: [19503]
[2023-05-30 15:24:23,427] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO CodeGenerator: Code generated in 18.9756 ms
[2023-05-30 15:24:23,438] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-27/datascience_20230527.json, range: 0-18079, partition values: [19504]
[2023-05-30 15:24:23,447] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-24/datascience_20230524.json, range: 0-9003, partition values: [19501]
[2023-05-30 15:24:23,451] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-8881, partition values: [19502]
[2023-05-30 15:24:23,458] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-28/datascience_20230528.json, range: 0-4475, partition values: [19505]
[2023-05-30 15:24:23,464] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileOutputCommitter: Saved output of task 'attempt_202305301524234589695090083256438_0002_m_000000_2' to file:/home/raf/pipeline-transform/datalake/silver/twitter_datascience/user/process_date=2023-05-27/_temporary/0/task_202305301524234589695090083256438_0002_m_000000
[2023-05-30 15:24:23,464] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO SparkHadoopMapRedUtil: attempt_202305301524234589695090083256438_0002_m_000000_2: Committed. Elapsed time: 0 ms.
[2023-05-30 15:24:23,466] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-05-30 15:24:23,469] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 119 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:24:23,471] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.143 s
[2023-05-30 15:24:23,471] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:24:23,472] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-05-30 15:24:23,472] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-05-30 15:24:23,472] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.150133 s
[2023-05-30 15:24:23,475] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileFormatWriter: Start to commit write Job 170217af-a7bb-4dad-af62-042f9eac37d1.
[2023-05-30 15:24:23,488] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileFormatWriter: Write Job 170217af-a7bb-4dad-af62-042f9eac37d1 committed. Elapsed time: 12 ms.
[2023-05-30 15:24:23,489] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO FileFormatWriter: Finished processing stats for write job 170217af-a7bb-4dad-af62-042f9eac37d1.
[2023-05-30 15:24:23,519] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO SparkContext: Invoking stop() from shutdown hook
[2023-05-30 15:24:23,533] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO SparkUI: Stopped Spark web UI at http://172.19.51.55:4040
[2023-05-30 15:24:23,549] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-05-30 15:24:23,560] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO MemoryStore: MemoryStore cleared
[2023-05-30 15:24:23,560] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO BlockManager: BlockManager stopped
[2023-05-30 15:24:23,563] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-05-30 15:24:23,566] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-05-30 15:24:23,572] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO SparkContext: Successfully stopped SparkContext
[2023-05-30 15:24:23,573] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO ShutdownHookManager: Shutdown hook called
[2023-05-30 15:24:23,574] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-a095e838-4c37-4b20-b6fe-ed950d7f3c6a/pyspark-5682f744-c12c-4314-8e61-324fbfc94d81
[2023-05-30 15:24:23,577] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-519c32c1-159b-4b21-8a3d-49938de3e901
[2023-05-30 15:24:23,579] {spark_submit.py:495} INFO - 23/05/30 15:24:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-a095e838-4c37-4b20-b6fe-ed950d7f3c6a
[2023-05-30 15:24:23,611] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230527T000000, start_date=20230530T182407, end_date=20230530T182423
[2023-05-30 15:24:23,691] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-30 15:24:23,700] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-30 15:55:24,961] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-27T00:00:00+00:00 [queued]>
[2023-05-30 15:55:24,970] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-27T00:00:00+00:00 [queued]>
[2023-05-30 15:55:24,970] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:55:24,970] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-30 15:55:24,970] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:55:24,993] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-05-27 00:00:00+00:00
[2023-05-30 15:55:24,996] {standard_task_runner.py:52} INFO - Started process 27500 to run task
[2023-05-30 15:55:24,998] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-05-27T00:00:00+00:00', '--job-id', '45', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpugz4m3v1', '--error-file', '/tmp/tmp_fp5gliv']
[2023-05-30 15:55:24,999] {standard_task_runner.py:80} INFO - Job 45: Subtask transform_twitter_datascience
[2023-05-30 15:55:25,040] {task_command.py:370} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-27T00:00:00+00:00 [running]> on host BIRIBA.localdomain
[2023-05-30 15:55:25,135] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=TwitterDAG
AIRFLOW_CTX_TASK_ID=transform_twitter_datascience
AIRFLOW_CTX_EXECUTION_DATE=2023-05-27T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-05-27T00:00:00+00:00
[2023-05-30 15:55:25,141] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2023-05-30 15:55:25,143] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/raf/pipeline-transform/src/spark/transformation.py --src /home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-27 --dest /home/raf/pipeline-transform/datalake/silver/twitter_datascience/extract_date=2023-05-27 --process-date 2023-05-27
[2023-05-30 15:55:27,472] {spark_submit.py:495} INFO - 23/05/30 15:55:27 WARN Utils: Your hostname, BIRIBA resolves to a loopback address: 127.0.1.1; using 172.19.51.55 instead (on interface eth0)
[2023-05-30 15:55:27,476] {spark_submit.py:495} INFO - 23/05/30 15:55:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-05-30 15:55:29,457] {spark_submit.py:495} INFO - 23/05/30 15:55:29 INFO SparkContext: Running Spark version 3.3.1
[2023-05-30 15:55:29,546] {spark_submit.py:495} INFO - 23/05/30 15:55:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-05-30 15:55:29,671] {spark_submit.py:495} INFO - 23/05/30 15:55:29 INFO ResourceUtils: ==============================================================
[2023-05-30 15:55:29,671] {spark_submit.py:495} INFO - 23/05/30 15:55:29 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-05-30 15:55:29,672] {spark_submit.py:495} INFO - 23/05/30 15:55:29 INFO ResourceUtils: ==============================================================
[2023-05-30 15:55:29,672] {spark_submit.py:495} INFO - 23/05/30 15:55:29 INFO SparkContext: Submitted application: twitter_transformation
[2023-05-30 15:55:29,703] {spark_submit.py:495} INFO - 23/05/30 15:55:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-05-30 15:55:29,716] {spark_submit.py:495} INFO - 23/05/30 15:55:29 INFO ResourceProfile: Limiting resource is cpu
[2023-05-30 15:55:29,718] {spark_submit.py:495} INFO - 23/05/30 15:55:29 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-05-30 15:55:29,781] {spark_submit.py:495} INFO - 23/05/30 15:55:29 INFO SecurityManager: Changing view acls to: raf
[2023-05-30 15:55:29,782] {spark_submit.py:495} INFO - 23/05/30 15:55:29 INFO SecurityManager: Changing modify acls to: raf
[2023-05-30 15:55:29,783] {spark_submit.py:495} INFO - 23/05/30 15:55:29 INFO SecurityManager: Changing view acls groups to:
[2023-05-30 15:55:29,784] {spark_submit.py:495} INFO - 23/05/30 15:55:29 INFO SecurityManager: Changing modify acls groups to:
[2023-05-30 15:55:29,784] {spark_submit.py:495} INFO - 23/05/30 15:55:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(raf); groups with view permissions: Set(); users  with modify permissions: Set(raf); groups with modify permissions: Set()
[2023-05-30 15:55:30,150] {spark_submit.py:495} INFO - 23/05/30 15:55:30 INFO Utils: Successfully started service 'sparkDriver' on port 38445.
[2023-05-30 15:55:30,183] {spark_submit.py:495} INFO - 23/05/30 15:55:30 INFO SparkEnv: Registering MapOutputTracker
[2023-05-30 15:55:30,231] {spark_submit.py:495} INFO - 23/05/30 15:55:30 INFO SparkEnv: Registering BlockManagerMaster
[2023-05-30 15:55:30,261] {spark_submit.py:495} INFO - 23/05/30 15:55:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-05-30 15:55:30,263] {spark_submit.py:495} INFO - 23/05/30 15:55:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-05-30 15:55:30,268] {spark_submit.py:495} INFO - 23/05/30 15:55:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-05-30 15:55:30,333] {spark_submit.py:495} INFO - 23/05/30 15:55:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1f435c02-b48a-42b3-bc4f-43302a4847a9
[2023-05-30 15:55:30,364] {spark_submit.py:495} INFO - 23/05/30 15:55:30 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2023-05-30 15:55:30,417] {spark_submit.py:495} INFO - 23/05/30 15:55:30 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-05-30 15:55:30,745] {spark_submit.py:495} INFO - 23/05/30 15:55:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-05-30 15:55:31,057] {spark_submit.py:495} INFO - 23/05/30 15:55:31 INFO Executor: Starting executor ID driver on host 172.19.51.55
[2023-05-30 15:55:31,073] {spark_submit.py:495} INFO - 23/05/30 15:55:31 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-05-30 15:55:31,115] {spark_submit.py:495} INFO - 23/05/30 15:55:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33447.
[2023-05-30 15:55:31,115] {spark_submit.py:495} INFO - 23/05/30 15:55:31 INFO NettyBlockTransferService: Server created on 172.19.51.55:33447
[2023-05-30 15:55:31,118] {spark_submit.py:495} INFO - 23/05/30 15:55:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-05-30 15:55:31,131] {spark_submit.py:495} INFO - 23/05/30 15:55:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.19.51.55, 33447, None)
[2023-05-30 15:55:31,136] {spark_submit.py:495} INFO - 23/05/30 15:55:31 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.51.55:33447 with 366.3 MiB RAM, BlockManagerId(driver, 172.19.51.55, 33447, None)
[2023-05-30 15:55:31,140] {spark_submit.py:495} INFO - 23/05/30 15:55:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.19.51.55, 33447, None)
[2023-05-30 15:55:31,146] {spark_submit.py:495} INFO - 23/05/30 15:55:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.19.51.55, 33447, None)
[2023-05-30 15:55:32,013] {spark_submit.py:495} INFO - 23/05/30 15:55:32 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-05-30 15:55:32,021] {spark_submit.py:495} INFO - 23/05/30 15:55:32 INFO SharedState: Warehouse path is 'file:/home/raf/pipeline-transform/airflow_pipeline/spark-warehouse'.
[2023-05-30 15:55:33,514] {spark_submit.py:495} INFO - 23/05/30 15:55:33 INFO InMemoryFileIndex: It took 42 ms to list leaf files for 1 paths.
[2023-05-30 15:55:33,602] {spark_submit.py:495} INFO - 23/05/30 15:55:33 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2023-05-30 15:55:36,621] {spark_submit.py:495} INFO - 23/05/30 15:55:36 INFO FileSourceStrategy: Pushed Filters:
[2023-05-30 15:55:36,624] {spark_submit.py:495} INFO - 23/05/30 15:55:36 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-30 15:55:36,629] {spark_submit.py:495} INFO - 23/05/30 15:55:36 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-05-30 15:55:37,035] {spark_submit.py:495} INFO - 23/05/30 15:55:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 349.6 KiB, free 366.0 MiB)
[2023-05-30 15:55:37,109] {spark_submit.py:495} INFO - 23/05/30 15:55:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 365.9 MiB)
[2023-05-30 15:55:37,113] {spark_submit.py:495} INFO - 23/05/30 15:55:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.51.55:33447 (size: 33.9 KiB, free: 366.3 MiB)
[2023-05-30 15:55:37,119] {spark_submit.py:495} INFO - 23/05/30 15:55:37 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:55:37,131] {spark_submit.py:495} INFO - 23/05/30 15:55:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198746 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:55:37,492] {spark_submit.py:495} INFO - 23/05/30 15:55:37 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:55:37,517] {spark_submit.py:495} INFO - 23/05/30 15:55:37 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:55:37,518] {spark_submit.py:495} INFO - 23/05/30 15:55:37 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:55:37,519] {spark_submit.py:495} INFO - 23/05/30 15:55:37 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:55:37,520] {spark_submit.py:495} INFO - 23/05/30 15:55:37 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:55:37,525] {spark_submit.py:495} INFO - 23/05/30 15:55:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:55:37,639] {spark_submit.py:495} INFO - 23/05/30 15:55:37 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 365.9 MiB)
[2023-05-30 15:55:37,644] {spark_submit.py:495} INFO - 23/05/30 15:55:37 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 365.9 MiB)
[2023-05-30 15:55:37,646] {spark_submit.py:495} INFO - 23/05/30 15:55:37 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.51.55:33447 (size: 7.0 KiB, free: 366.3 MiB)
[2023-05-30 15:55:37,648] {spark_submit.py:495} INFO - 23/05/30 15:55:37 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:55:37,670] {spark_submit.py:495} INFO - 23/05/30 15:55:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:55:37,671] {spark_submit.py:495} INFO - 23/05/30 15:55:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-05-30 15:55:37,740] {spark_submit.py:495} INFO - 23/05/30 15:55:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 4992 bytes) taskResourceAssignments Map()
[2023-05-30 15:55:37,761] {spark_submit.py:495} INFO - 23/05/30 15:55:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-05-30 15:55:38,295] {spark_submit.py:495} INFO - 23/05/30 15:55:38 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-27/datascience_20230527.json, range: 0-4442, partition values: [empty row]
[2023-05-30 15:55:38,596] {spark_submit.py:495} INFO - 23/05/30 15:55:38 INFO CodeGenerator: Code generated in 227.2096 ms
[2023-05-30 15:55:38,719] {spark_submit.py:495} INFO - 23/05/30 15:55:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
[2023-05-30 15:55:38,738] {spark_submit.py:495} INFO - 23/05/30 15:55:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1011 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:55:38,740] {spark_submit.py:495} INFO - 23/05/30 15:55:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-05-30 15:55:38,750] {spark_submit.py:495} INFO - 23/05/30 15:55:38 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.206 s
[2023-05-30 15:55:38,764] {spark_submit.py:495} INFO - 23/05/30 15:55:38 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:55:38,766] {spark_submit.py:495} INFO - 23/05/30 15:55:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-05-30 15:55:38,793] {spark_submit.py:495} INFO - 23/05/30 15:55:38 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.295448 s
[2023-05-30 15:55:39,355] {spark_submit.py:495} INFO - 23/05/30 15:55:39 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-05-30 15:55:39,359] {spark_submit.py:495} INFO - 23/05/30 15:55:39 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-05-30 15:55:39,365] {spark_submit.py:495} INFO - 23/05/30 15:55:39 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-05-30 15:55:39,500] {spark_submit.py:495} INFO - 23/05/30 15:55:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:55:39,500] {spark_submit.py:495} INFO - 23/05/30 15:55:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:55:39,501] {spark_submit.py:495} INFO - 23/05/30 15:55:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:55:39,825] {spark_submit.py:495} INFO - 23/05/30 15:55:39 INFO CodeGenerator: Code generated in 208.4064 ms
[2023-05-30 15:55:39,839] {spark_submit.py:495} INFO - 23/05/30 15:55:39 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 349.5 KiB, free 365.6 MiB)
[2023-05-30 15:55:39,859] {spark_submit.py:495} INFO - 23/05/30 15:55:39 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.5 MiB)
[2023-05-30 15:55:39,865] {spark_submit.py:495} INFO - 23/05/30 15:55:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.51.55:33447 (size: 33.8 KiB, free: 366.2 MiB)
[2023-05-30 15:55:39,868] {spark_submit.py:495} INFO - 23/05/30 15:55:39 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:55:39,873] {spark_submit.py:495} INFO - 23/05/30 15:55:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198746 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:55:40,005] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:55:40,007] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:55:40,007] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:55:40,008] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:55:40,008] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:55:40,013] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:55:40,063] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.6 KiB, free 365.3 MiB)
[2023-05-30 15:55:40,067] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 81.6 KiB, free 365.2 MiB)
[2023-05-30 15:55:40,068] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.19.51.55:33447 (size: 81.6 KiB, free: 366.1 MiB)
[2023-05-30 15:55:40,069] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:55:40,071] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:55:40,071] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-05-30 15:55:40,079] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 5221 bytes) taskResourceAssignments Map()
[2023-05-30 15:55:40,086] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-05-30 15:55:40,167] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:55:40,167] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:55:40,172] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:55:40,226] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-27/datascience_20230527.json, range: 0-4442, partition values: [empty row]
[2023-05-30 15:55:40,287] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO CodeGenerator: Code generated in 53.6446 ms
[2023-05-30 15:55:40,322] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO CodeGenerator: Code generated in 7.8067 ms
[2023-05-30 15:55:40,459] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO FileOutputCommitter: Saved output of task 'attempt_202305301555393144892134336158723_0001_m_000000_1' to file:/home/raf/pipeline-transform/datalake/silver/twitter_datascience/extract_date=2023-05-27/tweet/process_date=2023-05-27/_temporary/0/task_202305301555393144892134336158723_0001_m_000000
[2023-05-30 15:55:40,462] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO SparkHadoopMapRedUtil: attempt_202305301555393144892134336158723_0001_m_000000_1: Committed. Elapsed time: 2 ms.
[2023-05-30 15:55:40,475] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-05-30 15:55:40,488] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 411 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:55:40,488] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-05-30 15:55:40,488] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.476 s
[2023-05-30 15:55:40,488] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:55:40,488] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-05-30 15:55:40,488] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0.483545 s
[2023-05-30 15:55:40,491] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO FileFormatWriter: Start to commit write Job 9039a57c-1c06-41da-add0-a8768c505796.
[2023-05-30 15:55:40,540] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO FileFormatWriter: Write Job 9039a57c-1c06-41da-add0-a8768c505796 committed. Elapsed time: 47 ms.
[2023-05-30 15:55:40,572] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO FileFormatWriter: Finished processing stats for write job 9039a57c-1c06-41da-add0-a8768c505796.
[2023-05-30 15:55:40,674] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO FileSourceStrategy: Pushed Filters:
[2023-05-30 15:55:40,674] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-30 15:55:40,675] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-05-30 15:55:40,699] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:55:40,700] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:55:40,700] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:55:40,764] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO CodeGenerator: Code generated in 22.6321 ms
[2023-05-30 15:55:40,772] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 349.5 KiB, free 364.9 MiB)
[2023-05-30 15:55:40,791] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 364.8 MiB)
[2023-05-30 15:55:40,792] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.19.51.55:33447 (size: 33.8 KiB, free: 366.1 MiB)
[2023-05-30 15:55:40,793] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:55:40,797] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198746 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:55:40,827] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:55:40,828] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:55:40,828] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:55:40,828] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:55:40,829] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:55:40,832] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:55:40,859] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.7 KiB, free 364.6 MiB)
[2023-05-30 15:55:40,863] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.3 KiB, free 364.6 MiB)
[2023-05-30 15:55:40,864] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.19.51.55:33447 (size: 76.3 KiB, free: 366.0 MiB)
[2023-05-30 15:55:40,865] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:55:40,866] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:55:40,867] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-05-30 15:55:40,868] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 5221 bytes) taskResourceAssignments Map()
[2023-05-30 15:55:40,869] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-05-30 15:55:40,900] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:55:40,900] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:55:40,901] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:55:40,964] {spark_submit.py:495} INFO - 23/05/30 15:55:40 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-27/datascience_20230527.json, range: 0-4442, partition values: [empty row]
[2023-05-30 15:55:41,002] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO CodeGenerator: Code generated in 34.2988 ms
[2023-05-30 15:55:41,015] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO FileOutputCommitter: Saved output of task 'attempt_202305301555402799989442432489363_0002_m_000000_2' to file:/home/raf/pipeline-transform/datalake/silver/twitter_datascience/extract_date=2023-05-27/user/process_date=2023-05-27/_temporary/0/task_202305301555402799989442432489363_0002_m_000000
[2023-05-30 15:55:41,015] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO SparkHadoopMapRedUtil: attempt_202305301555402799989442432489363_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-05-30 15:55:41,018] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-05-30 15:55:41,020] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 151 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:55:41,020] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-05-30 15:55:41,021] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.188 s
[2023-05-30 15:55:41,022] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:55:41,022] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-05-30 15:55:41,022] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.195473 s
[2023-05-30 15:55:41,024] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO FileFormatWriter: Start to commit write Job 5106502c-3c8b-4590-a631-46fa1653b239.
[2023-05-30 15:55:41,044] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO FileFormatWriter: Write Job 5106502c-3c8b-4590-a631-46fa1653b239 committed. Elapsed time: 20 ms.
[2023-05-30 15:55:41,047] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO FileFormatWriter: Finished processing stats for write job 5106502c-3c8b-4590-a631-46fa1653b239.
[2023-05-30 15:55:41,106] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO SparkContext: Invoking stop() from shutdown hook
[2023-05-30 15:55:41,136] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO SparkUI: Stopped Spark web UI at http://172.19.51.55:4040
[2023-05-30 15:55:41,148] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.19.51.55:33447 in memory (size: 7.0 KiB, free: 366.0 MiB)
[2023-05-30 15:55:41,170] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-05-30 15:55:41,185] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO MemoryStore: MemoryStore cleared
[2023-05-30 15:55:41,186] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO BlockManager: BlockManager stopped
[2023-05-30 15:55:41,193] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-05-30 15:55:41,201] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-05-30 15:55:41,205] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO SparkContext: Successfully stopped SparkContext
[2023-05-30 15:55:41,206] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO ShutdownHookManager: Shutdown hook called
[2023-05-30 15:55:41,206] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-663d3bd3-132f-4a8b-831e-acae7a713a18
[2023-05-30 15:55:41,209] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-c9dda796-5bea-46de-bc16-f0be1548839f/pyspark-dee46fea-2f74-4601-a8d9-5efccdf8cd2c
[2023-05-30 15:55:41,212] {spark_submit.py:495} INFO - 23/05/30 15:55:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-c9dda796-5bea-46de-bc16-f0be1548839f
[2023-05-30 15:55:41,255] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230527T000000, start_date=20230530T185524, end_date=20230530T185541
[2023-05-30 15:55:41,299] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-30 15:55:41,323] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
