[2023-05-30 15:06:30,378] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-25T00:00:00+00:00 [queued]>
[2023-05-30 15:06:30,384] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-25T00:00:00+00:00 [queued]>
[2023-05-30 15:06:30,384] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:06:30,384] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-30 15:06:30,384] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:06:30,413] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-05-25 00:00:00+00:00
[2023-05-30 15:06:30,415] {standard_task_runner.py:52} INFO - Started process 14982 to run task
[2023-05-30 15:06:30,418] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-05-25T00:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpwl2lw6ao', '--error-file', '/tmp/tmp1n2k5wxp']
[2023-05-30 15:06:30,419] {standard_task_runner.py:80} INFO - Job 26: Subtask transform_twitter_datascience
[2023-05-30 15:06:30,463] {task_command.py:370} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-25T00:00:00+00:00 [running]> on host BIRIBA.localdomain
[2023-05-30 15:06:30,524] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=TwitterDAG
AIRFLOW_CTX_TASK_ID=transform_twitter_datascience
AIRFLOW_CTX_EXECUTION_DATE=2023-05-25T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-05-25T00:00:00+00:00
[2023-05-30 15:06:30,539] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2023-05-30 15:06:30,541] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/aluno/Documents/curso2/src/spark/transformation.py --src curso2/datalake/twitter_datascience --dest curso2/Silver/twitter_datascience --process-date 2023-05-25
[2023-05-30 15:06:32,330] {spark_submit.py:495} INFO - 23/05/30 15:06:32 WARN Utils: Your hostname, BIRIBA resolves to a loopback address: 127.0.1.1; using 172.19.51.55 instead (on interface eth0)
[2023-05-30 15:06:32,335] {spark_submit.py:495} INFO - 23/05/30 15:06:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-05-30 15:06:33,893] {spark_submit.py:495} INFO - python3: can't open file '/home/aluno/Documents/curso2/src/spark/transformation.py': [Errno 2] No such file or directory
[2023-05-30 15:06:33,900] {spark_submit.py:495} INFO - 23/05/30 15:06:33 INFO ShutdownHookManager: Shutdown hook called
[2023-05-30 15:06:33,901] {spark_submit.py:495} INFO - 23/05/30 15:06:33 INFO ShutdownHookManager: Deleting directory /tmp/spark-c4f2dc0b-1b64-436d-91cb-e6a9e82e78ca
[2023-05-30 15:06:33,947] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/raf/pipeline/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/raf/pipeline/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name twitter_transformation --queue root.default /home/aluno/Documents/curso2/src/spark/transformation.py --src curso2/datalake/twitter_datascience --dest curso2/Silver/twitter_datascience --process-date 2023-05-25. Error code is: 2.
[2023-05-30 15:06:33,950] {taskinstance.py:1395} INFO - Marking task as FAILED. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230525T000000, start_date=20230530T180630, end_date=20230530T180633
[2023-05-30 15:06:33,981] {standard_task_runner.py:92} ERROR - Failed to execute job 26 for task transform_twitter_datascience (Cannot execute: spark-submit --master local --name twitter_transformation --queue root.default /home/aluno/Documents/curso2/src/spark/transformation.py --src curso2/datalake/twitter_datascience --dest curso2/Silver/twitter_datascience --process-date 2023-05-25. Error code is: 2.; 14982)
[2023-05-30 15:06:34,008] {local_task_job.py:156} INFO - Task exited with return code 1
[2023-05-30 15:06:34,026] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-30 15:11:40,965] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-25T00:00:00+00:00 [queued]>
[2023-05-30 15:11:40,975] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-25T00:00:00+00:00 [queued]>
[2023-05-30 15:11:40,975] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:11:40,975] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-30 15:11:40,976] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:11:41,006] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-05-25 00:00:00+00:00
[2023-05-30 15:11:41,010] {standard_task_runner.py:52} INFO - Started process 17553 to run task
[2023-05-30 15:11:41,012] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-05-25T00:00:00+00:00', '--job-id', '26', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpip7d7i6n', '--error-file', '/tmp/tmph1jz51t2']
[2023-05-30 15:11:41,013] {standard_task_runner.py:80} INFO - Job 26: Subtask transform_twitter_datascience
[2023-05-30 15:11:41,053] {task_command.py:370} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-25T00:00:00+00:00 [running]> on host BIRIBA.localdomain
[2023-05-30 15:11:41,108] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=TwitterDAG
AIRFLOW_CTX_TASK_ID=transform_twitter_datascience
AIRFLOW_CTX_EXECUTION_DATE=2023-05-25T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-05-25T00:00:00+00:00
[2023-05-30 15:11:41,112] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2023-05-30 15:11:41,113] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/raf/pipeline-transform/src/spark/transformation.py --src curso2/datalake/twitter_datascience --dest curso2/Silver/twitter_datascience --process-date 2023-05-25
[2023-05-30 15:11:43,006] {spark_submit.py:495} INFO - 23/05/30 15:11:43 WARN Utils: Your hostname, BIRIBA resolves to a loopback address: 127.0.1.1; using 172.19.51.55 instead (on interface eth0)
[2023-05-30 15:11:43,013] {spark_submit.py:495} INFO - 23/05/30 15:11:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-05-30 15:11:45,039] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO SparkContext: Running Spark version 3.3.1
[2023-05-30 15:11:45,129] {spark_submit.py:495} INFO - 23/05/30 15:11:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-05-30 15:11:45,221] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO ResourceUtils: ==============================================================
[2023-05-30 15:11:45,221] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-05-30 15:11:45,222] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO ResourceUtils: ==============================================================
[2023-05-30 15:11:45,223] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO SparkContext: Submitted application: twitter_transformation
[2023-05-30 15:11:45,261] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-05-30 15:11:45,278] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO ResourceProfile: Limiting resource is cpu
[2023-05-30 15:11:45,279] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-05-30 15:11:45,345] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO SecurityManager: Changing view acls to: raf
[2023-05-30 15:11:45,346] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO SecurityManager: Changing modify acls to: raf
[2023-05-30 15:11:45,347] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO SecurityManager: Changing view acls groups to:
[2023-05-30 15:11:45,348] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO SecurityManager: Changing modify acls groups to:
[2023-05-30 15:11:45,348] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(raf); groups with view permissions: Set(); users  with modify permissions: Set(raf); groups with modify permissions: Set()
[2023-05-30 15:11:45,671] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO Utils: Successfully started service 'sparkDriver' on port 34767.
[2023-05-30 15:11:45,702] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO SparkEnv: Registering MapOutputTracker
[2023-05-30 15:11:45,767] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO SparkEnv: Registering BlockManagerMaster
[2023-05-30 15:11:45,798] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-05-30 15:11:45,800] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-05-30 15:11:45,811] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-05-30 15:11:45,918] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2f647644-8edf-47e7-a507-fd549f5b8307
[2023-05-30 15:11:45,949] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2023-05-30 15:11:45,986] {spark_submit.py:495} INFO - 23/05/30 15:11:45 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-05-30 15:11:46,328] {spark_submit.py:495} INFO - 23/05/30 15:11:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-05-30 15:11:46,479] {spark_submit.py:495} INFO - 23/05/30 15:11:46 INFO Executor: Starting executor ID driver on host 172.19.51.55
[2023-05-30 15:11:46,488] {spark_submit.py:495} INFO - 23/05/30 15:11:46 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-05-30 15:11:46,522] {spark_submit.py:495} INFO - 23/05/30 15:11:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41007.
[2023-05-30 15:11:46,522] {spark_submit.py:495} INFO - 23/05/30 15:11:46 INFO NettyBlockTransferService: Server created on 172.19.51.55:41007
[2023-05-30 15:11:46,527] {spark_submit.py:495} INFO - 23/05/30 15:11:46 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-05-30 15:11:46,535] {spark_submit.py:495} INFO - 23/05/30 15:11:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.19.51.55, 41007, None)
[2023-05-30 15:11:46,539] {spark_submit.py:495} INFO - 23/05/30 15:11:46 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.51.55:41007 with 366.3 MiB RAM, BlockManagerId(driver, 172.19.51.55, 41007, None)
[2023-05-30 15:11:46,543] {spark_submit.py:495} INFO - 23/05/30 15:11:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.19.51.55, 41007, None)
[2023-05-30 15:11:46,545] {spark_submit.py:495} INFO - 23/05/30 15:11:46 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.19.51.55, 41007, None)
[2023-05-30 15:11:47,203] {spark_submit.py:495} INFO - 23/05/30 15:11:47 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-05-30 15:11:47,213] {spark_submit.py:495} INFO - 23/05/30 15:11:47 INFO SharedState: Warehouse path is 'file:/home/raf/pipeline-transform/airflow_pipeline/spark-warehouse'.
[2023-05-30 15:11:48,395] {spark_submit.py:495} INFO - 23/05/30 15:11:48 INFO InMemoryFileIndex: It took 50 ms to list leaf files for 1 paths.
[2023-05-30 15:11:48,549] {spark_submit.py:495} INFO - 23/05/30 15:11:48 INFO InMemoryFileIndex: It took 9 ms to list leaf files for 6 paths.
[2023-05-30 15:11:51,225] {spark_submit.py:495} INFO - 23/05/30 15:11:51 INFO FileSourceStrategy: Pushed Filters:
[2023-05-30 15:11:51,229] {spark_submit.py:495} INFO - 23/05/30 15:11:51 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-30 15:11:51,235] {spark_submit.py:495} INFO - 23/05/30 15:11:51 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-05-30 15:11:51,646] {spark_submit.py:495} INFO - 23/05/30 15:11:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 349.6 KiB, free 366.0 MiB)
[2023-05-30 15:11:51,740] {spark_submit.py:495} INFO - 23/05/30 15:11:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 365.9 MiB)
[2023-05-30 15:11:51,745] {spark_submit.py:495} INFO - 23/05/30 15:11:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.51.55:41007 (size: 33.9 KiB, free: 366.3 MiB)
[2023-05-30 15:11:51,754] {spark_submit.py:495} INFO - 23/05/30 15:11:51 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:11:51,895] {spark_submit.py:495} INFO - 23/05/30 15:11:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25192964 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:11:52,138] {spark_submit.py:495} INFO - 23/05/30 15:11:52 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:11:52,165] {spark_submit.py:495} INFO - 23/05/30 15:11:52 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:11:52,166] {spark_submit.py:495} INFO - 23/05/30 15:11:52 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:11:52,166] {spark_submit.py:495} INFO - 23/05/30 15:11:52 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:11:52,168] {spark_submit.py:495} INFO - 23/05/30 15:11:52 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:11:52,172] {spark_submit.py:495} INFO - 23/05/30 15:11:52 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:11:52,299] {spark_submit.py:495} INFO - 23/05/30 15:11:52 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 365.9 MiB)
[2023-05-30 15:11:52,303] {spark_submit.py:495} INFO - 23/05/30 15:11:52 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 365.9 MiB)
[2023-05-30 15:11:52,304] {spark_submit.py:495} INFO - 23/05/30 15:11:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.51.55:41007 (size: 7.0 KiB, free: 366.3 MiB)
[2023-05-30 15:11:52,305] {spark_submit.py:495} INFO - 23/05/30 15:11:52 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:11:52,326] {spark_submit.py:495} INFO - 23/05/30 15:11:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:11:52,329] {spark_submit.py:495} INFO - 23/05/30 15:11:52 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-05-30 15:11:52,406] {spark_submit.py:495} INFO - 23/05/30 15:11:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 5929 bytes) taskResourceAssignments Map()
[2023-05-30 15:11:52,425] {spark_submit.py:495} INFO - 23/05/30 15:11:52 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-05-30 15:11:52,816] {spark_submit.py:495} INFO - 23/05/30 15:11:52 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-28/datascience_20230528.json, range: 0-4545, partition values: [empty row]
[2023-05-30 15:11:53,115] {spark_submit.py:495} INFO - 23/05/30 15:11:53 INFO CodeGenerator: Code generated in 235.5402 ms
[2023-05-30 15:11:53,184] {spark_submit.py:495} INFO - 23/05/30 15:11:53 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-29/datascience_20230529.json, range: 0-4545, partition values: [empty row]
[2023-05-30 15:11:53,196] {spark_submit.py:495} INFO - 23/05/30 15:11:53 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-27/datascience_20230527.json, range: 0-4542, partition values: [empty row]
[2023-05-30 15:11:53,205] {spark_submit.py:495} INFO - 23/05/30 15:11:53 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-24/datascience_20230524.json, range: 0-4518, partition values: [empty row]
[2023-05-30 15:11:53,208] {spark_submit.py:495} INFO - 23/05/30 15:11:53 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-26/datascience_20230526.json, range: 0-4506, partition values: [empty row]
[2023-05-30 15:11:53,218] {spark_submit.py:495} INFO - 23/05/30 15:11:53 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-4484, partition values: [empty row]
[2023-05-30 15:11:53,262] {spark_submit.py:495} INFO - 23/05/30 15:11:53 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
[2023-05-30 15:11:53,281] {spark_submit.py:495} INFO - 23/05/30 15:11:53 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 894 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:11:53,283] {spark_submit.py:495} INFO - 23/05/30 15:11:53 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-05-30 15:11:53,289] {spark_submit.py:495} INFO - 23/05/30 15:11:53 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.098 s
[2023-05-30 15:11:53,298] {spark_submit.py:495} INFO - 23/05/30 15:11:53 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:11:53,300] {spark_submit.py:495} INFO - 23/05/30 15:11:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-05-30 15:11:53,304] {spark_submit.py:495} INFO - 23/05/30 15:11:53 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.165324 s
[2023-05-30 15:11:53,922] {spark_submit.py:495} INFO - 23/05/30 15:11:53 INFO DataSourceStrategy: Pruning directories with:
[2023-05-30 15:11:53,926] {spark_submit.py:495} INFO - 23/05/30 15:11:53 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-05-30 15:11:53,928] {spark_submit.py:495} INFO - 23/05/30 15:11:53 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-05-30 15:11:53,929] {spark_submit.py:495} INFO - 23/05/30 15:11:53 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-05-30 15:11:54,057] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:11:54,058] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:11:54,060] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:11:54,130] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.19.51.55:41007 in memory (size: 7.0 KiB, free: 366.3 MiB)
[2023-05-30 15:11:54,410] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO CodeGenerator: Code generated in 195.1924 ms
[2023-05-30 15:11:54,426] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 349.5 KiB, free 365.6 MiB)
[2023-05-30 15:11:54,452] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.6 MiB)
[2023-05-30 15:11:54,455] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.51.55:41007 (size: 33.8 KiB, free: 366.2 MiB)
[2023-05-30 15:11:54,458] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:11:54,466] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25192964 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:11:54,572] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:11:54,575] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:11:54,576] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:11:54,576] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:11:54,576] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:11:54,581] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:11:54,634] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.2 KiB, free 365.3 MiB)
[2023-05-30 15:11:54,638] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.2 KiB, free 365.2 MiB)
[2023-05-30 15:11:54,639] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.19.51.55:41007 (size: 83.2 KiB, free: 366.2 MiB)
[2023-05-30 15:11:54,640] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:11:54,641] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:11:54,641] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-05-30 15:11:54,649] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 6273 bytes) taskResourceAssignments Map()
[2023-05-30 15:11:54,650] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-05-30 15:11:54,727] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:11:54,727] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:11:54,727] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:11:54,769] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-28/datascience_20230528.json, range: 0-4545, partition values: [19505]
[2023-05-30 15:11:54,813] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO CodeGenerator: Code generated in 39.1976 ms
[2023-05-30 15:11:54,852] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO CodeGenerator: Code generated in 10.5898 ms
[2023-05-30 15:11:54,885] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-29/datascience_20230529.json, range: 0-4545, partition values: [19506]
[2023-05-30 15:11:54,893] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-27/datascience_20230527.json, range: 0-4542, partition values: [19504]
[2023-05-30 15:11:54,903] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-24/datascience_20230524.json, range: 0-4518, partition values: [19501]
[2023-05-30 15:11:54,911] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-26/datascience_20230526.json, range: 0-4506, partition values: [19503]
[2023-05-30 15:11:54,920] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-4484, partition values: [19502]
[2023-05-30 15:11:54,950] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO FileOutputCommitter: Saved output of task 'attempt_202305301511547657631121891113448_0001_m_000000_1' to file:/home/raf/pipeline-transform/airflow_pipeline/curso2/Silver/twitter_datascience/tweet/process_date=2023-05-25/_temporary/0/task_202305301511547657631121891113448_0001_m_000000
[2023-05-30 15:11:54,952] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO SparkHadoopMapRedUtil: attempt_202305301511547657631121891113448_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-05-30 15:11:54,963] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-05-30 15:11:54,966] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 322 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:11:54,969] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.388 s
[2023-05-30 15:11:54,969] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:11:54,969] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-05-30 15:11:54,970] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-05-30 15:11:54,971] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0.399046 s
[2023-05-30 15:11:54,973] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO FileFormatWriter: Start to commit write Job 7c35247a-147e-43e7-ae5a-9d620ee184f7.
[2023-05-30 15:11:54,993] {spark_submit.py:495} INFO - 23/05/30 15:11:54 INFO FileFormatWriter: Write Job 7c35247a-147e-43e7-ae5a-9d620ee184f7 committed. Elapsed time: 18 ms.
[2023-05-30 15:11:55,001] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO FileFormatWriter: Finished processing stats for write job 7c35247a-147e-43e7-ae5a-9d620ee184f7.
[2023-05-30 15:11:55,048] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO DataSourceStrategy: Pruning directories with:
[2023-05-30 15:11:55,049] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO FileSourceStrategy: Pushed Filters:
[2023-05-30 15:11:55,049] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-30 15:11:55,050] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-05-30 15:11:55,064] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:11:55,064] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:11:55,065] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:11:55,118] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO CodeGenerator: Code generated in 21.6143 ms
[2023-05-30 15:11:55,125] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 349.5 KiB, free 364.9 MiB)
[2023-05-30 15:11:55,140] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 364.9 MiB)
[2023-05-30 15:11:55,141] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.19.51.55:41007 (size: 33.8 KiB, free: 366.1 MiB)
[2023-05-30 15:11:55,143] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:11:55,144] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25192964 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:11:55,176] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:11:55,180] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:11:55,180] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:11:55,180] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:11:55,180] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:11:55,182] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:11:55,206] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.2 KiB, free 364.7 MiB)
[2023-05-30 15:11:55,209] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.5 KiB, free 364.6 MiB)
[2023-05-30 15:11:55,213] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.19.51.55:41007 (size: 76.5 KiB, free: 366.0 MiB)
[2023-05-30 15:11:55,214] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:11:55,216] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:11:55,217] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-05-30 15:11:55,218] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 6273 bytes) taskResourceAssignments Map()
[2023-05-30 15:11:55,219] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-05-30 15:11:55,239] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:11:55,239] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:11:55,240] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:11:55,274] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-28/datascience_20230528.json, range: 0-4545, partition values: [19505]
[2023-05-30 15:11:55,304] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO CodeGenerator: Code generated in 23.8052 ms
[2023-05-30 15:11:55,312] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-29/datascience_20230529.json, range: 0-4545, partition values: [19506]
[2023-05-30 15:11:55,318] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-27/datascience_20230527.json, range: 0-4542, partition values: [19504]
[2023-05-30 15:11:55,323] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-24/datascience_20230524.json, range: 0-4518, partition values: [19501]
[2023-05-30 15:11:55,329] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-26/datascience_20230526.json, range: 0-4506, partition values: [19503]
[2023-05-30 15:11:55,335] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-4484, partition values: [19502]
[2023-05-30 15:11:55,346] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO FileOutputCommitter: Saved output of task 'attempt_202305301511555001306975853342548_0002_m_000000_2' to file:/home/raf/pipeline-transform/airflow_pipeline/curso2/Silver/twitter_datascience/user/process_date=2023-05-25/_temporary/0/task_202305301511555001306975853342548_0002_m_000000
[2023-05-30 15:11:55,346] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO SparkHadoopMapRedUtil: attempt_202305301511555001306975853342548_0002_m_000000_2: Committed. Elapsed time: 2 ms.
[2023-05-30 15:11:55,348] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-05-30 15:11:55,358] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 140 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:11:55,359] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-05-30 15:11:55,363] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.175 s
[2023-05-30 15:11:55,363] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:11:55,364] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-05-30 15:11:55,366] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.183837 s
[2023-05-30 15:11:55,366] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO FileFormatWriter: Start to commit write Job ba8e3578-1030-4b75-a7fb-82fc908e92e6.
[2023-05-30 15:11:55,409] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO FileFormatWriter: Write Job ba8e3578-1030-4b75-a7fb-82fc908e92e6 committed. Elapsed time: 47 ms.
[2023-05-30 15:11:55,410] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO FileFormatWriter: Finished processing stats for write job ba8e3578-1030-4b75-a7fb-82fc908e92e6.
[2023-05-30 15:11:55,451] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO SparkContext: Invoking stop() from shutdown hook
[2023-05-30 15:11:55,471] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO SparkUI: Stopped Spark web UI at http://172.19.51.55:4040
[2023-05-30 15:11:55,498] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-05-30 15:11:55,509] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO MemoryStore: MemoryStore cleared
[2023-05-30 15:11:55,510] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO BlockManager: BlockManager stopped
[2023-05-30 15:11:55,520] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-05-30 15:11:55,524] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-05-30 15:11:55,533] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO SparkContext: Successfully stopped SparkContext
[2023-05-30 15:11:55,533] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO ShutdownHookManager: Shutdown hook called
[2023-05-30 15:11:55,534] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-718fd865-acd7-4704-af7a-6712e8eb4af8/pyspark-ad495df1-20e9-4aa4-acd2-9f48a41d87e0
[2023-05-30 15:11:55,537] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-1b9e2768-0702-444f-afca-c5a3c0dd134d
[2023-05-30 15:11:55,540] {spark_submit.py:495} INFO - 23/05/30 15:11:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-718fd865-acd7-4704-af7a-6712e8eb4af8
[2023-05-30 15:11:55,588] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230525T000000, start_date=20230530T181140, end_date=20230530T181155
[2023-05-30 15:11:55,636] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-30 15:11:55,646] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-30 15:23:25,324] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-25T00:00:00+00:00 [queued]>
[2023-05-30 15:23:25,333] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-25T00:00:00+00:00 [queued]>
[2023-05-30 15:23:25,333] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:23:25,333] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-30 15:23:25,333] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:23:25,355] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-05-25 00:00:00+00:00
[2023-05-30 15:23:25,357] {standard_task_runner.py:52} INFO - Started process 21136 to run task
[2023-05-30 15:23:25,360] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-05-25T00:00:00+00:00', '--job-id', '40', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp2ukdc53c', '--error-file', '/tmp/tmpi5ahte_h']
[2023-05-30 15:23:25,360] {standard_task_runner.py:80} INFO - Job 40: Subtask transform_twitter_datascience
[2023-05-30 15:23:25,407] {task_command.py:370} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-25T00:00:00+00:00 [running]> on host BIRIBA.localdomain
[2023-05-30 15:23:25,465] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=TwitterDAG
AIRFLOW_CTX_TASK_ID=transform_twitter_datascience
AIRFLOW_CTX_EXECUTION_DATE=2023-05-25T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-05-25T00:00:00+00:00
[2023-05-30 15:23:25,471] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2023-05-30 15:23:25,472] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/raf/pipeline-transform/src/spark/transformation.py --src /home/raf/pipeline-transform/datalake/bronze/twitter_datascience --dest /home/raf/pipeline-transform/datalake/silver/twitter_datascience --process-date 2023-05-25
[2023-05-30 15:23:27,344] {spark_submit.py:495} INFO - 23/05/30 15:23:27 WARN Utils: Your hostname, BIRIBA resolves to a loopback address: 127.0.1.1; using 172.19.51.55 instead (on interface eth0)
[2023-05-30 15:23:27,347] {spark_submit.py:495} INFO - 23/05/30 15:23:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-05-30 15:23:29,398] {spark_submit.py:495} INFO - 23/05/30 15:23:29 INFO SparkContext: Running Spark version 3.3.1
[2023-05-30 15:23:29,472] {spark_submit.py:495} INFO - 23/05/30 15:23:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-05-30 15:23:29,560] {spark_submit.py:495} INFO - 23/05/30 15:23:29 INFO ResourceUtils: ==============================================================
[2023-05-30 15:23:29,560] {spark_submit.py:495} INFO - 23/05/30 15:23:29 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-05-30 15:23:29,561] {spark_submit.py:495} INFO - 23/05/30 15:23:29 INFO ResourceUtils: ==============================================================
[2023-05-30 15:23:29,561] {spark_submit.py:495} INFO - 23/05/30 15:23:29 INFO SparkContext: Submitted application: twitter_transformation
[2023-05-30 15:23:29,602] {spark_submit.py:495} INFO - 23/05/30 15:23:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-05-30 15:23:29,618] {spark_submit.py:495} INFO - 23/05/30 15:23:29 INFO ResourceProfile: Limiting resource is cpu
[2023-05-30 15:23:29,621] {spark_submit.py:495} INFO - 23/05/30 15:23:29 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-05-30 15:23:29,691] {spark_submit.py:495} INFO - 23/05/30 15:23:29 INFO SecurityManager: Changing view acls to: raf
[2023-05-30 15:23:29,692] {spark_submit.py:495} INFO - 23/05/30 15:23:29 INFO SecurityManager: Changing modify acls to: raf
[2023-05-30 15:23:29,696] {spark_submit.py:495} INFO - 23/05/30 15:23:29 INFO SecurityManager: Changing view acls groups to:
[2023-05-30 15:23:29,697] {spark_submit.py:495} INFO - 23/05/30 15:23:29 INFO SecurityManager: Changing modify acls groups to:
[2023-05-30 15:23:29,698] {spark_submit.py:495} INFO - 23/05/30 15:23:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(raf); groups with view permissions: Set(); users  with modify permissions: Set(raf); groups with modify permissions: Set()
[2023-05-30 15:23:30,003] {spark_submit.py:495} INFO - 23/05/30 15:23:30 INFO Utils: Successfully started service 'sparkDriver' on port 44411.
[2023-05-30 15:23:30,045] {spark_submit.py:495} INFO - 23/05/30 15:23:30 INFO SparkEnv: Registering MapOutputTracker
[2023-05-30 15:23:30,102] {spark_submit.py:495} INFO - 23/05/30 15:23:30 INFO SparkEnv: Registering BlockManagerMaster
[2023-05-30 15:23:30,143] {spark_submit.py:495} INFO - 23/05/30 15:23:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-05-30 15:23:30,144] {spark_submit.py:495} INFO - 23/05/30 15:23:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-05-30 15:23:30,157] {spark_submit.py:495} INFO - 23/05/30 15:23:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-05-30 15:23:30,230] {spark_submit.py:495} INFO - 23/05/30 15:23:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ad0f66ae-1663-4089-a183-4bc8e04f90ec
[2023-05-30 15:23:30,257] {spark_submit.py:495} INFO - 23/05/30 15:23:30 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2023-05-30 15:23:30,280] {spark_submit.py:495} INFO - 23/05/30 15:23:30 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-05-30 15:23:30,600] {spark_submit.py:495} INFO - 23/05/30 15:23:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-05-30 15:23:30,794] {spark_submit.py:495} INFO - 23/05/30 15:23:30 INFO Executor: Starting executor ID driver on host 172.19.51.55
[2023-05-30 15:23:30,805] {spark_submit.py:495} INFO - 23/05/30 15:23:30 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-05-30 15:23:30,834] {spark_submit.py:495} INFO - 23/05/30 15:23:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43567.
[2023-05-30 15:23:30,835] {spark_submit.py:495} INFO - 23/05/30 15:23:30 INFO NettyBlockTransferService: Server created on 172.19.51.55:43567
[2023-05-30 15:23:30,837] {spark_submit.py:495} INFO - 23/05/30 15:23:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-05-30 15:23:30,844] {spark_submit.py:495} INFO - 23/05/30 15:23:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.19.51.55, 43567, None)
[2023-05-30 15:23:30,850] {spark_submit.py:495} INFO - 23/05/30 15:23:30 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.51.55:43567 with 366.3 MiB RAM, BlockManagerId(driver, 172.19.51.55, 43567, None)
[2023-05-30 15:23:30,852] {spark_submit.py:495} INFO - 23/05/30 15:23:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.19.51.55, 43567, None)
[2023-05-30 15:23:30,854] {spark_submit.py:495} INFO - 23/05/30 15:23:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.19.51.55, 43567, None)
[2023-05-30 15:23:31,546] {spark_submit.py:495} INFO - 23/05/30 15:23:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-05-30 15:23:31,558] {spark_submit.py:495} INFO - 23/05/30 15:23:31 INFO SharedState: Warehouse path is 'file:/home/raf/pipeline-transform/airflow_pipeline/spark-warehouse'.
[2023-05-30 15:23:32,808] {spark_submit.py:495} INFO - 23/05/30 15:23:32 INFO InMemoryFileIndex: It took 64 ms to list leaf files for 1 paths.
[2023-05-30 15:23:32,960] {spark_submit.py:495} INFO - 23/05/30 15:23:32 INFO InMemoryFileIndex: It took 6 ms to list leaf files for 3 paths.
[2023-05-30 15:23:35,923] {spark_submit.py:495} INFO - 23/05/30 15:23:35 INFO FileSourceStrategy: Pushed Filters:
[2023-05-30 15:23:35,925] {spark_submit.py:495} INFO - 23/05/30 15:23:35 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-30 15:23:35,928] {spark_submit.py:495} INFO - 23/05/30 15:23:35 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-05-30 15:23:36,341] {spark_submit.py:495} INFO - 23/05/30 15:23:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 349.6 KiB, free 366.0 MiB)
[2023-05-30 15:23:36,407] {spark_submit.py:495} INFO - 23/05/30 15:23:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 365.9 MiB)
[2023-05-30 15:23:36,411] {spark_submit.py:495} INFO - 23/05/30 15:23:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.51.55:43567 (size: 33.9 KiB, free: 366.3 MiB)
[2023-05-30 15:23:36,416] {spark_submit.py:495} INFO - 23/05/30 15:23:36 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:23:36,554] {spark_submit.py:495} INFO - 23/05/30 15:23:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12623885 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:23:36,792] {spark_submit.py:495} INFO - 23/05/30 15:23:36 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:23:36,815] {spark_submit.py:495} INFO - 23/05/30 15:23:36 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:23:36,815] {spark_submit.py:495} INFO - 23/05/30 15:23:36 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:23:36,816] {spark_submit.py:495} INFO - 23/05/30 15:23:36 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:23:36,817] {spark_submit.py:495} INFO - 23/05/30 15:23:36 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:23:36,822] {spark_submit.py:495} INFO - 23/05/30 15:23:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:23:36,926] {spark_submit.py:495} INFO - 23/05/30 15:23:36 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 365.9 MiB)
[2023-05-30 15:23:36,929] {spark_submit.py:495} INFO - 23/05/30 15:23:36 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 365.9 MiB)
[2023-05-30 15:23:36,930] {spark_submit.py:495} INFO - 23/05/30 15:23:36 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.51.55:43567 (size: 7.0 KiB, free: 366.3 MiB)
[2023-05-30 15:23:36,931] {spark_submit.py:495} INFO - 23/05/30 15:23:36 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:23:36,945] {spark_submit.py:495} INFO - 23/05/30 15:23:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:23:36,947] {spark_submit.py:495} INFO - 23/05/30 15:23:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-05-30 15:23:37,001] {spark_submit.py:495} INFO - 23/05/30 15:23:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 5326 bytes) taskResourceAssignments Map()
[2023-05-30 15:23:37,020] {spark_submit.py:495} INFO - 23/05/30 15:23:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-05-30 15:23:37,399] {spark_submit.py:495} INFO - 23/05/30 15:23:37 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-26/datascience_20230526.json, range: 0-23089, partition values: [empty row]
[2023-05-30 15:23:37,662] {spark_submit.py:495} INFO - 23/05/30 15:23:37 INFO CodeGenerator: Code generated in 193.9431 ms
[2023-05-30 15:23:37,735] {spark_submit.py:495} INFO - 23/05/30 15:23:37 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-24/datascience_20230524.json, range: 0-9003, partition values: [empty row]
[2023-05-30 15:23:37,751] {spark_submit.py:495} INFO - 23/05/30 15:23:37 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-8881, partition values: [empty row]
[2023-05-30 15:23:37,777] {spark_submit.py:495} INFO - 23/05/30 15:23:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-05-30 15:23:37,794] {spark_submit.py:495} INFO - 23/05/30 15:23:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 799 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:23:37,799] {spark_submit.py:495} INFO - 23/05/30 15:23:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-05-30 15:23:37,806] {spark_submit.py:495} INFO - 23/05/30 15:23:37 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0.964 s
[2023-05-30 15:23:37,814] {spark_submit.py:495} INFO - 23/05/30 15:23:37 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:23:37,815] {spark_submit.py:495} INFO - 23/05/30 15:23:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-05-30 15:23:37,820] {spark_submit.py:495} INFO - 23/05/30 15:23:37 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.025628 s
[2023-05-30 15:23:38,543] {spark_submit.py:495} INFO - 23/05/30 15:23:38 INFO DataSourceStrategy: Pruning directories with:
[2023-05-30 15:23:38,551] {spark_submit.py:495} INFO - 23/05/30 15:23:38 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-05-30 15:23:38,554] {spark_submit.py:495} INFO - 23/05/30 15:23:38 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-05-30 15:23:38,555] {spark_submit.py:495} INFO - 23/05/30 15:23:38 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-05-30 15:23:38,681] {spark_submit.py:495} INFO - 23/05/30 15:23:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:23:38,681] {spark_submit.py:495} INFO - 23/05/30 15:23:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:23:38,683] {spark_submit.py:495} INFO - 23/05/30 15:23:38 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:23:38,979] {spark_submit.py:495} INFO - 23/05/30 15:23:38 INFO CodeGenerator: Code generated in 187.9586 ms
[2023-05-30 15:23:38,987] {spark_submit.py:495} INFO - 23/05/30 15:23:38 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 349.5 KiB, free 365.6 MiB)
[2023-05-30 15:23:39,001] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.5 MiB)
[2023-05-30 15:23:39,002] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.51.55:43567 (size: 33.8 KiB, free: 366.2 MiB)
[2023-05-30 15:23:39,003] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:23:39,010] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12623885 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:23:39,110] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:23:39,113] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:23:39,113] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:23:39,114] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:23:39,114] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:23:39,115] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:23:39,157] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.1 KiB, free 365.3 MiB)
[2023-05-30 15:23:39,182] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.3 KiB, free 365.2 MiB)
[2023-05-30 15:23:39,183] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.19.51.55:43567 (size: 83.3 KiB, free: 366.1 MiB)
[2023-05-30 15:23:39,185] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:23:39,187] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:23:39,188] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-05-30 15:23:39,201] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 5607 bytes) taskResourceAssignments Map()
[2023-05-30 15:23:39,202] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-05-30 15:23:39,218] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.19.51.55:43567 in memory (size: 7.0 KiB, free: 366.2 MiB)
[2023-05-30 15:23:39,325] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:23:39,326] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:23:39,327] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:23:39,415] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-26/datascience_20230526.json, range: 0-23089, partition values: [19503]
[2023-05-30 15:23:39,474] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO CodeGenerator: Code generated in 52.7933 ms
[2023-05-30 15:23:39,538] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO CodeGenerator: Code generated in 12.4719 ms
[2023-05-30 15:23:39,612] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-24/datascience_20230524.json, range: 0-9003, partition values: [19501]
[2023-05-30 15:23:39,628] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-8881, partition values: [19502]
[2023-05-30 15:23:39,661] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO FileOutputCommitter: Saved output of task 'attempt_202305301523399065053950676035232_0001_m_000000_1' to file:/home/raf/pipeline-transform/datalake/silver/twitter_datascience/tweet/process_date=2023-05-25/_temporary/0/task_202305301523399065053950676035232_0001_m_000000
[2023-05-30 15:23:39,662] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO SparkHadoopMapRedUtil: attempt_202305301523399065053950676035232_0001_m_000000_1: Committed. Elapsed time: 5 ms.
[2023-05-30 15:23:39,672] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-05-30 15:23:39,677] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 483 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:23:39,677] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-05-30 15:23:39,678] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.562 s
[2023-05-30 15:23:39,678] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:23:39,679] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-05-30 15:23:39,680] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0.568732 s
[2023-05-30 15:23:39,683] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO FileFormatWriter: Start to commit write Job 330dd081-ad63-4bc0-b4fb-622310728f7c.
[2023-05-30 15:23:39,701] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO FileFormatWriter: Write Job 330dd081-ad63-4bc0-b4fb-622310728f7c committed. Elapsed time: 16 ms.
[2023-05-30 15:23:39,704] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO FileFormatWriter: Finished processing stats for write job 330dd081-ad63-4bc0-b4fb-622310728f7c.
[2023-05-30 15:23:39,764] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO DataSourceStrategy: Pruning directories with:
[2023-05-30 15:23:39,764] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO FileSourceStrategy: Pushed Filters:
[2023-05-30 15:23:39,765] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-30 15:23:39,765] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-05-30 15:23:39,779] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:23:39,779] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:23:39,779] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:23:39,828] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO CodeGenerator: Code generated in 24.0491 ms
[2023-05-30 15:23:39,838] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 349.5 KiB, free 364.9 MiB)
[2023-05-30 15:23:39,859] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 364.9 MiB)
[2023-05-30 15:23:39,862] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.19.51.55:43567 (size: 33.8 KiB, free: 366.1 MiB)
[2023-05-30 15:23:39,864] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:23:39,867] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 12623885 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:23:39,903] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:23:39,909] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:23:39,909] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:23:39,910] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:23:39,910] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:23:39,913] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:23:39,949] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.1 KiB, free 364.7 MiB)
[2023-05-30 15:23:39,953] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.4 KiB, free 364.6 MiB)
[2023-05-30 15:23:39,956] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.19.51.55:43567 (size: 76.4 KiB, free: 366.0 MiB)
[2023-05-30 15:23:39,959] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:23:39,960] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:23:39,960] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-05-30 15:23:39,962] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 5607 bytes) taskResourceAssignments Map()
[2023-05-30 15:23:39,966] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-05-30 15:23:39,992] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:23:39,992] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:23:39,994] {spark_submit.py:495} INFO - 23/05/30 15:23:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:23:40,028] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-26/datascience_20230526.json, range: 0-23089, partition values: [19503]
[2023-05-30 15:23:40,054] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO CodeGenerator: Code generated in 21.3934 ms
[2023-05-30 15:23:40,068] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-24/datascience_20230524.json, range: 0-9003, partition values: [19501]
[2023-05-30 15:23:40,079] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-8881, partition values: [19502]
[2023-05-30 15:23:40,087] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO FileOutputCommitter: Saved output of task 'attempt_202305301523398930276358540481218_0002_m_000000_2' to file:/home/raf/pipeline-transform/datalake/silver/twitter_datascience/user/process_date=2023-05-25/_temporary/0/task_202305301523398930276358540481218_0002_m_000000
[2023-05-30 15:23:40,088] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO SparkHadoopMapRedUtil: attempt_202305301523398930276358540481218_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-05-30 15:23:40,094] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-05-30 15:23:40,100] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 139 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:23:40,100] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-05-30 15:23:40,103] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.186 s
[2023-05-30 15:23:40,104] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:23:40,104] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-05-30 15:23:40,104] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.198490 s
[2023-05-30 15:23:40,104] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO FileFormatWriter: Start to commit write Job 3e13b12f-913d-4ff4-9f33-a47d39030a06.
[2023-05-30 15:23:40,121] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO FileFormatWriter: Write Job 3e13b12f-913d-4ff4-9f33-a47d39030a06 committed. Elapsed time: 17 ms.
[2023-05-30 15:23:40,121] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO FileFormatWriter: Finished processing stats for write job 3e13b12f-913d-4ff4-9f33-a47d39030a06.
[2023-05-30 15:23:40,154] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO SparkContext: Invoking stop() from shutdown hook
[2023-05-30 15:23:40,168] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO SparkUI: Stopped Spark web UI at http://172.19.51.55:4040
[2023-05-30 15:23:40,186] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-05-30 15:23:40,197] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO MemoryStore: MemoryStore cleared
[2023-05-30 15:23:40,198] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO BlockManager: BlockManager stopped
[2023-05-30 15:23:40,201] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-05-30 15:23:40,204] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-05-30 15:23:40,209] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO SparkContext: Successfully stopped SparkContext
[2023-05-30 15:23:40,210] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO ShutdownHookManager: Shutdown hook called
[2023-05-30 15:23:40,211] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-90523d99-9880-451e-b0eb-225de62485fa/pyspark-ea5ba5ed-f6f0-488c-a81b-b6b5b4b39482
[2023-05-30 15:23:40,213] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-87bf151a-cf4b-4f0f-921b-fcec237c5675
[2023-05-30 15:23:40,216] {spark_submit.py:495} INFO - 23/05/30 15:23:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-90523d99-9880-451e-b0eb-225de62485fa
[2023-05-30 15:23:40,261] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230525T000000, start_date=20230530T182325, end_date=20230530T182340
[2023-05-30 15:23:40,301] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-30 15:23:40,312] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-30 15:54:07,568] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-25T00:00:00+00:00 [queued]>
[2023-05-30 15:54:07,574] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-25T00:00:00+00:00 [queued]>
[2023-05-30 15:54:07,574] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:54:07,574] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-30 15:54:07,574] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:54:07,596] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-05-25 00:00:00+00:00
[2023-05-30 15:54:07,599] {standard_task_runner.py:52} INFO - Started process 26119 to run task
[2023-05-30 15:54:07,602] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-05-25T00:00:00+00:00', '--job-id', '40', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpab9isjas', '--error-file', '/tmp/tmp_al9qfxg']
[2023-05-30 15:54:07,603] {standard_task_runner.py:80} INFO - Job 40: Subtask transform_twitter_datascience
[2023-05-30 15:54:07,643] {task_command.py:370} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-25T00:00:00+00:00 [running]> on host BIRIBA.localdomain
[2023-05-30 15:54:07,705] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=TwitterDAG
AIRFLOW_CTX_TASK_ID=transform_twitter_datascience
AIRFLOW_CTX_EXECUTION_DATE=2023-05-25T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-05-25T00:00:00+00:00
[2023-05-30 15:54:07,712] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2023-05-30 15:54:07,714] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/raf/pipeline-transform/src/spark/transformation.py --src /home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-25 --dest /home/raf/pipeline-transform/datalake/silver/twitter_datascience/extract_date=2023-05-25 --process-date 2023-05-25
[2023-05-30 15:54:10,018] {spark_submit.py:495} INFO - 23/05/30 15:54:10 WARN Utils: Your hostname, BIRIBA resolves to a loopback address: 127.0.1.1; using 172.19.51.55 instead (on interface eth0)
[2023-05-30 15:54:10,023] {spark_submit.py:495} INFO - 23/05/30 15:54:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-05-30 15:54:12,377] {spark_submit.py:495} INFO - 23/05/30 15:54:12 INFO SparkContext: Running Spark version 3.3.1
[2023-05-30 15:54:12,487] {spark_submit.py:495} INFO - 23/05/30 15:54:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-05-30 15:54:12,642] {spark_submit.py:495} INFO - 23/05/30 15:54:12 INFO ResourceUtils: ==============================================================
[2023-05-30 15:54:12,642] {spark_submit.py:495} INFO - 23/05/30 15:54:12 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-05-30 15:54:12,643] {spark_submit.py:495} INFO - 23/05/30 15:54:12 INFO ResourceUtils: ==============================================================
[2023-05-30 15:54:12,644] {spark_submit.py:495} INFO - 23/05/30 15:54:12 INFO SparkContext: Submitted application: twitter_transformation
[2023-05-30 15:54:12,673] {spark_submit.py:495} INFO - 23/05/30 15:54:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-05-30 15:54:12,687] {spark_submit.py:495} INFO - 23/05/30 15:54:12 INFO ResourceProfile: Limiting resource is cpu
[2023-05-30 15:54:12,689] {spark_submit.py:495} INFO - 23/05/30 15:54:12 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-05-30 15:54:12,755] {spark_submit.py:495} INFO - 23/05/30 15:54:12 INFO SecurityManager: Changing view acls to: raf
[2023-05-30 15:54:12,756] {spark_submit.py:495} INFO - 23/05/30 15:54:12 INFO SecurityManager: Changing modify acls to: raf
[2023-05-30 15:54:12,757] {spark_submit.py:495} INFO - 23/05/30 15:54:12 INFO SecurityManager: Changing view acls groups to:
[2023-05-30 15:54:12,757] {spark_submit.py:495} INFO - 23/05/30 15:54:12 INFO SecurityManager: Changing modify acls groups to:
[2023-05-30 15:54:12,758] {spark_submit.py:495} INFO - 23/05/30 15:54:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(raf); groups with view permissions: Set(); users  with modify permissions: Set(raf); groups with modify permissions: Set()
[2023-05-30 15:54:13,237] {spark_submit.py:495} INFO - 23/05/30 15:54:13 INFO Utils: Successfully started service 'sparkDriver' on port 38809.
[2023-05-30 15:54:13,327] {spark_submit.py:495} INFO - 23/05/30 15:54:13 INFO SparkEnv: Registering MapOutputTracker
[2023-05-30 15:54:13,390] {spark_submit.py:495} INFO - 23/05/30 15:54:13 INFO SparkEnv: Registering BlockManagerMaster
[2023-05-30 15:54:13,421] {spark_submit.py:495} INFO - 23/05/30 15:54:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-05-30 15:54:13,422] {spark_submit.py:495} INFO - 23/05/30 15:54:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-05-30 15:54:13,430] {spark_submit.py:495} INFO - 23/05/30 15:54:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-05-30 15:54:13,499] {spark_submit.py:495} INFO - 23/05/30 15:54:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f9163f2b-60c7-4025-a602-869fac2cf6b7
[2023-05-30 15:54:13,535] {spark_submit.py:495} INFO - 23/05/30 15:54:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2023-05-30 15:54:13,562] {spark_submit.py:495} INFO - 23/05/30 15:54:13 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-05-30 15:54:13,930] {spark_submit.py:495} INFO - 23/05/30 15:54:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-05-30 15:54:14,206] {spark_submit.py:495} INFO - 23/05/30 15:54:14 INFO Executor: Starting executor ID driver on host 172.19.51.55
[2023-05-30 15:54:14,231] {spark_submit.py:495} INFO - 23/05/30 15:54:14 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-05-30 15:54:14,304] {spark_submit.py:495} INFO - 23/05/30 15:54:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40673.
[2023-05-30 15:54:14,305] {spark_submit.py:495} INFO - 23/05/30 15:54:14 INFO NettyBlockTransferService: Server created on 172.19.51.55:40673
[2023-05-30 15:54:14,309] {spark_submit.py:495} INFO - 23/05/30 15:54:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-05-30 15:54:14,325] {spark_submit.py:495} INFO - 23/05/30 15:54:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.19.51.55, 40673, None)
[2023-05-30 15:54:14,331] {spark_submit.py:495} INFO - 23/05/30 15:54:14 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.51.55:40673 with 366.3 MiB RAM, BlockManagerId(driver, 172.19.51.55, 40673, None)
[2023-05-30 15:54:14,336] {spark_submit.py:495} INFO - 23/05/30 15:54:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.19.51.55, 40673, None)
[2023-05-30 15:54:14,341] {spark_submit.py:495} INFO - 23/05/30 15:54:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.19.51.55, 40673, None)
[2023-05-30 15:54:15,133] {spark_submit.py:495} INFO - 23/05/30 15:54:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-05-30 15:54:15,142] {spark_submit.py:495} INFO - 23/05/30 15:54:15 INFO SharedState: Warehouse path is 'file:/home/raf/pipeline-transform/airflow_pipeline/spark-warehouse'.
[2023-05-30 15:54:16,294] {spark_submit.py:495} INFO - 23/05/30 15:54:16 INFO InMemoryFileIndex: It took 41 ms to list leaf files for 1 paths.
[2023-05-30 15:54:16,373] {spark_submit.py:495} INFO - 23/05/30 15:54:16 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
[2023-05-30 15:54:20,011] {spark_submit.py:495} INFO - 23/05/30 15:54:20 INFO FileSourceStrategy: Pushed Filters:
[2023-05-30 15:54:20,014] {spark_submit.py:495} INFO - 23/05/30 15:54:20 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-30 15:54:20,020] {spark_submit.py:495} INFO - 23/05/30 15:54:20 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-05-30 15:54:20,385] {spark_submit.py:495} INFO - 23/05/30 15:54:20 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 349.6 KiB, free 366.0 MiB)
[2023-05-30 15:54:20,454] {spark_submit.py:495} INFO - 23/05/30 15:54:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 365.9 MiB)
[2023-05-30 15:54:20,457] {spark_submit.py:495} INFO - 23/05/30 15:54:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.51.55:40673 (size: 33.9 KiB, free: 366.3 MiB)
[2023-05-30 15:54:20,463] {spark_submit.py:495} INFO - 23/05/30 15:54:20 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:54:20,476] {spark_submit.py:495} INFO - 23/05/30 15:54:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203436 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:54:20,814] {spark_submit.py:495} INFO - 23/05/30 15:54:20 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:54:20,852] {spark_submit.py:495} INFO - 23/05/30 15:54:20 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:54:20,853] {spark_submit.py:495} INFO - 23/05/30 15:54:20 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:54:20,854] {spark_submit.py:495} INFO - 23/05/30 15:54:20 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:54:20,856] {spark_submit.py:495} INFO - 23/05/30 15:54:20 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:54:20,864] {spark_submit.py:495} INFO - 23/05/30 15:54:20 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:54:20,982] {spark_submit.py:495} INFO - 23/05/30 15:54:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 365.9 MiB)
[2023-05-30 15:54:20,987] {spark_submit.py:495} INFO - 23/05/30 15:54:20 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 365.9 MiB)
[2023-05-30 15:54:20,988] {spark_submit.py:495} INFO - 23/05/30 15:54:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.51.55:40673 (size: 7.0 KiB, free: 366.3 MiB)
[2023-05-30 15:54:20,989] {spark_submit.py:495} INFO - 23/05/30 15:54:20 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:54:21,007] {spark_submit.py:495} INFO - 23/05/30 15:54:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:54:21,008] {spark_submit.py:495} INFO - 23/05/30 15:54:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-05-30 15:54:21,061] {spark_submit.py:495} INFO - 23/05/30 15:54:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 4992 bytes) taskResourceAssignments Map()
[2023-05-30 15:54:21,079] {spark_submit.py:495} INFO - 23/05/30 15:54:21 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-05-30 15:54:21,554] {spark_submit.py:495} INFO - 23/05/30 15:54:21 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-9132, partition values: [empty row]
[2023-05-30 15:54:21,929] {spark_submit.py:495} INFO - 23/05/30 15:54:21 INFO CodeGenerator: Code generated in 303.4432 ms
[2023-05-30 15:54:22,025] {spark_submit.py:495} INFO - 23/05/30 15:54:22 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2849 bytes result sent to driver
[2023-05-30 15:54:22,038] {spark_submit.py:495} INFO - 23/05/30 15:54:22 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 988 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:54:22,040] {spark_submit.py:495} INFO - 23/05/30 15:54:22 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-05-30 15:54:22,046] {spark_submit.py:495} INFO - 23/05/30 15:54:22 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.161 s
[2023-05-30 15:54:22,055] {spark_submit.py:495} INFO - 23/05/30 15:54:22 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:54:22,056] {spark_submit.py:495} INFO - 23/05/30 15:54:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-05-30 15:54:22,079] {spark_submit.py:495} INFO - 23/05/30 15:54:22 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.264483 s
[2023-05-30 15:54:22,833] {spark_submit.py:495} INFO - 23/05/30 15:54:22 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-05-30 15:54:22,838] {spark_submit.py:495} INFO - 23/05/30 15:54:22 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-05-30 15:54:22,839] {spark_submit.py:495} INFO - 23/05/30 15:54:22 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-05-30 15:54:23,125] {spark_submit.py:495} INFO - 23/05/30 15:54:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:54:23,131] {spark_submit.py:495} INFO - 23/05/30 15:54:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:54:23,137] {spark_submit.py:495} INFO - 23/05/30 15:54:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:54:23,727] {spark_submit.py:495} INFO - 23/05/30 15:54:23 INFO CodeGenerator: Code generated in 396.1813 ms
[2023-05-30 15:54:23,748] {spark_submit.py:495} INFO - 23/05/30 15:54:23 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 349.5 KiB, free 365.6 MiB)
[2023-05-30 15:54:23,770] {spark_submit.py:495} INFO - 23/05/30 15:54:23 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.5 MiB)
[2023-05-30 15:54:23,773] {spark_submit.py:495} INFO - 23/05/30 15:54:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.51.55:40673 (size: 33.8 KiB, free: 366.2 MiB)
[2023-05-30 15:54:23,775] {spark_submit.py:495} INFO - 23/05/30 15:54:23 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:54:23,783] {spark_submit.py:495} INFO - 23/05/30 15:54:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203436 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:54:23,936] {spark_submit.py:495} INFO - 23/05/30 15:54:23 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:54:23,940] {spark_submit.py:495} INFO - 23/05/30 15:54:23 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:54:23,940] {spark_submit.py:495} INFO - 23/05/30 15:54:23 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:54:23,940] {spark_submit.py:495} INFO - 23/05/30 15:54:23 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:54:23,940] {spark_submit.py:495} INFO - 23/05/30 15:54:23 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:54:23,945] {spark_submit.py:495} INFO - 23/05/30 15:54:23 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:54:24,037] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.6 KiB, free 365.3 MiB)
[2023-05-30 15:54:24,047] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 81.6 KiB, free 365.2 MiB)
[2023-05-30 15:54:24,049] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.19.51.55:40673 (size: 81.6 KiB, free: 366.1 MiB)
[2023-05-30 15:54:24,053] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:54:24,054] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:54:24,054] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-05-30 15:54:24,071] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 5221 bytes) taskResourceAssignments Map()
[2023-05-30 15:54:24,072] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-05-30 15:54:24,229] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:54:24,230] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:54:24,231] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:54:24,350] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-9132, partition values: [empty row]
[2023-05-30 15:54:24,430] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO CodeGenerator: Code generated in 66.4314 ms
[2023-05-30 15:54:24,522] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO CodeGenerator: Code generated in 24.4637 ms
[2023-05-30 15:54:24,637] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO FileOutputCommitter: Saved output of task 'attempt_202305301554234608437322244383803_0001_m_000000_1' to file:/home/raf/pipeline-transform/datalake/silver/twitter_datascience/extract_date=2023-05-25/tweet/process_date=2023-05-25/_temporary/0/task_202305301554234608437322244383803_0001_m_000000
[2023-05-30 15:54:24,638] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO SparkHadoopMapRedUtil: attempt_202305301554234608437322244383803_0001_m_000000_1: Committed. Elapsed time: 2 ms.
[2023-05-30 15:54:24,652] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-05-30 15:54:24,656] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 600 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:54:24,657] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-05-30 15:54:24,667] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.710 s
[2023-05-30 15:54:24,667] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:54:24,667] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-05-30 15:54:24,676] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0.737092 s
[2023-05-30 15:54:24,677] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO FileFormatWriter: Start to commit write Job 1b71570c-c85a-46bf-97a7-09174653eba1.
[2023-05-30 15:54:24,723] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO FileFormatWriter: Write Job 1b71570c-c85a-46bf-97a7-09174653eba1 committed. Elapsed time: 44 ms.
[2023-05-30 15:54:24,730] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO FileFormatWriter: Finished processing stats for write job 1b71570c-c85a-46bf-97a7-09174653eba1.
[2023-05-30 15:54:24,796] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO FileSourceStrategy: Pushed Filters:
[2023-05-30 15:54:24,797] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-30 15:54:24,798] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-05-30 15:54:24,815] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:54:24,815] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:54:24,816] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:54:24,902] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO CodeGenerator: Code generated in 56.1295 ms
[2023-05-30 15:54:24,921] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 349.5 KiB, free 364.9 MiB)
[2023-05-30 15:54:24,946] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 364.8 MiB)
[2023-05-30 15:54:24,946] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.19.51.55:40673 (size: 33.8 KiB, free: 366.1 MiB)
[2023-05-30 15:54:24,948] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:54:24,950] {spark_submit.py:495} INFO - 23/05/30 15:54:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203436 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:54:25,060] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:54:25,062] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:54:25,062] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:54:25,062] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:54:25,062] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:54:25,071] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:54:25,126] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.19.51.55:40673 in memory (size: 33.8 KiB, free: 366.1 MiB)
[2023-05-30 15:54:25,132] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.7 KiB, free 364.7 MiB)
[2023-05-30 15:54:25,142] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.3 KiB, free 364.9 MiB)
[2023-05-30 15:54:25,144] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.19.51.55:40673 (size: 76.3 KiB, free: 366.1 MiB)
[2023-05-30 15:54:25,144] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:54:25,145] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:54:25,145] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-05-30 15:54:25,148] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 5221 bytes) taskResourceAssignments Map()
[2023-05-30 15:54:25,151] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-05-30 15:54:25,197] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:54:25,197] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:54:25,198] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:54:25,223] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.19.51.55:40673 in memory (size: 7.0 KiB, free: 366.1 MiB)
[2023-05-30 15:54:25,250] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.19.51.55:40673 in memory (size: 81.6 KiB, free: 366.2 MiB)
[2023-05-30 15:54:25,282] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-9132, partition values: [empty row]
[2023-05-30 15:54:25,312] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO CodeGenerator: Code generated in 23.3355 ms
[2023-05-30 15:54:25,332] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO FileOutputCommitter: Saved output of task 'attempt_202305301554245126550259492702959_0002_m_000000_2' to file:/home/raf/pipeline-transform/datalake/silver/twitter_datascience/extract_date=2023-05-25/user/process_date=2023-05-25/_temporary/0/task_202305301554245126550259492702959_0002_m_000000
[2023-05-30 15:54:25,333] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO SparkHadoopMapRedUtil: attempt_202305301554245126550259492702959_0002_m_000000_2: Committed. Elapsed time: 1 ms.
[2023-05-30 15:54:25,335] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-05-30 15:54:25,341] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 194 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:54:25,342] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-05-30 15:54:25,345] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.271 s
[2023-05-30 15:54:25,346] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:54:25,346] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-05-30 15:54:25,346] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.283919 s
[2023-05-30 15:54:25,346] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO FileFormatWriter: Start to commit write Job 2aa135ee-4be4-4444-9282-00d9cedfdda6.
[2023-05-30 15:54:25,364] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO FileFormatWriter: Write Job 2aa135ee-4be4-4444-9282-00d9cedfdda6 committed. Elapsed time: 19 ms.
[2023-05-30 15:54:25,370] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO FileFormatWriter: Finished processing stats for write job 2aa135ee-4be4-4444-9282-00d9cedfdda6.
[2023-05-30 15:54:25,412] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO SparkContext: Invoking stop() from shutdown hook
[2023-05-30 15:54:25,424] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO SparkUI: Stopped Spark web UI at http://172.19.51.55:4040
[2023-05-30 15:54:25,441] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-05-30 15:54:25,453] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO MemoryStore: MemoryStore cleared
[2023-05-30 15:54:25,454] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO BlockManager: BlockManager stopped
[2023-05-30 15:54:25,458] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-05-30 15:54:25,461] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-05-30 15:54:25,466] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO SparkContext: Successfully stopped SparkContext
[2023-05-30 15:54:25,466] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO ShutdownHookManager: Shutdown hook called
[2023-05-30 15:54:25,467] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-ed234ada-923e-442a-bcb9-9d5e6ff48b7a/pyspark-dfc5c75c-81e2-4aa1-912c-85267e00b136
[2023-05-30 15:54:25,471] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-b75cbb0f-25b7-43e7-827e-e79bd3194698
[2023-05-30 15:54:25,473] {spark_submit.py:495} INFO - 23/05/30 15:54:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-ed234ada-923e-442a-bcb9-9d5e6ff48b7a
[2023-05-30 15:54:25,532] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230525T000000, start_date=20230530T185407, end_date=20230530T185425
[2023-05-30 15:54:25,583] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-30 15:54:25,609] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
