[2023-05-30 15:06:55,742] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-28T00:00:00+00:00 [queued]>
[2023-05-30 15:06:55,747] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-28T00:00:00+00:00 [queued]>
[2023-05-30 15:06:55,747] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:06:55,747] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-30 15:06:55,747] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:06:55,767] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-05-28 00:00:00+00:00
[2023-05-30 15:06:55,769] {standard_task_runner.py:52} INFO - Started process 15418 to run task
[2023-05-30 15:06:55,772] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-05-28T00:00:00+00:00', '--job-id', '32', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmp_2_jybit', '--error-file', '/tmp/tmpaxxhan3s']
[2023-05-30 15:06:55,773] {standard_task_runner.py:80} INFO - Job 32: Subtask transform_twitter_datascience
[2023-05-30 15:06:55,817] {task_command.py:370} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-28T00:00:00+00:00 [running]> on host BIRIBA.localdomain
[2023-05-30 15:06:55,888] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=TwitterDAG
AIRFLOW_CTX_TASK_ID=transform_twitter_datascience
AIRFLOW_CTX_EXECUTION_DATE=2023-05-28T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-05-28T00:00:00+00:00
[2023-05-30 15:06:55,897] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2023-05-30 15:06:55,898] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/aluno/Documents/curso2/src/spark/transformation.py --src curso2/datalake/twitter_datascience --dest curso2/Silver/twitter_datascience --process-date 2023-05-28
[2023-05-30 15:06:58,207] {spark_submit.py:495} INFO - 23/05/30 15:06:58 WARN Utils: Your hostname, BIRIBA resolves to a loopback address: 127.0.1.1; using 172.19.51.55 instead (on interface eth0)
[2023-05-30 15:06:58,212] {spark_submit.py:495} INFO - 23/05/30 15:06:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-05-30 15:06:59,783] {spark_submit.py:495} INFO - python3: can't open file '/home/aluno/Documents/curso2/src/spark/transformation.py': [Errno 2] No such file or directory
[2023-05-30 15:06:59,791] {spark_submit.py:495} INFO - 23/05/30 15:06:59 INFO ShutdownHookManager: Shutdown hook called
[2023-05-30 15:06:59,794] {spark_submit.py:495} INFO - 23/05/30 15:06:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-ed9ffeb5-7250-4598-9ee6-8eb15010813f
[2023-05-30 15:06:59,817] {taskinstance.py:1889} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/raf/pipeline/lib/python3.9/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/raf/pipeline/lib/python3.9/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 426, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name twitter_transformation --queue root.default /home/aluno/Documents/curso2/src/spark/transformation.py --src curso2/datalake/twitter_datascience --dest curso2/Silver/twitter_datascience --process-date 2023-05-28. Error code is: 2.
[2023-05-30 15:06:59,820] {taskinstance.py:1395} INFO - Marking task as FAILED. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230528T000000, start_date=20230530T180655, end_date=20230530T180659
[2023-05-30 15:06:59,840] {standard_task_runner.py:92} ERROR - Failed to execute job 32 for task transform_twitter_datascience (Cannot execute: spark-submit --master local --name twitter_transformation --queue root.default /home/aluno/Documents/curso2/src/spark/transformation.py --src curso2/datalake/twitter_datascience --dest curso2/Silver/twitter_datascience --process-date 2023-05-28. Error code is: 2.; 15418)
[2023-05-30 15:06:59,875] {local_task_job.py:156} INFO - Task exited with return code 1
[2023-05-30 15:06:59,889] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-30 15:12:44,377] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-28T00:00:00+00:00 [queued]>
[2023-05-30 15:12:44,389] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-28T00:00:00+00:00 [queued]>
[2023-05-30 15:12:44,389] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:12:44,389] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-30 15:12:44,389] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:12:44,421] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-05-28 00:00:00+00:00
[2023-05-30 15:12:44,424] {standard_task_runner.py:52} INFO - Started process 18735 to run task
[2023-05-30 15:12:44,427] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-05-28T00:00:00+00:00', '--job-id', '32', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpk2t94b1n', '--error-file', '/tmp/tmpecce1j8a']
[2023-05-30 15:12:44,428] {standard_task_runner.py:80} INFO - Job 32: Subtask transform_twitter_datascience
[2023-05-30 15:12:44,482] {task_command.py:370} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-28T00:00:00+00:00 [running]> on host BIRIBA.localdomain
[2023-05-30 15:12:44,561] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=TwitterDAG
AIRFLOW_CTX_TASK_ID=transform_twitter_datascience
AIRFLOW_CTX_EXECUTION_DATE=2023-05-28T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-05-28T00:00:00+00:00
[2023-05-30 15:12:44,569] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2023-05-30 15:12:44,570] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/raf/pipeline-transform/src/spark/transformation.py --src curso2/datalake/twitter_datascience --dest curso2/Silver/twitter_datascience --process-date 2023-05-28
[2023-05-30 15:12:46,558] {spark_submit.py:495} INFO - 23/05/30 15:12:46 WARN Utils: Your hostname, BIRIBA resolves to a loopback address: 127.0.1.1; using 172.19.51.55 instead (on interface eth0)
[2023-05-30 15:12:46,561] {spark_submit.py:495} INFO - 23/05/30 15:12:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-05-30 15:12:48,790] {spark_submit.py:495} INFO - 23/05/30 15:12:48 INFO SparkContext: Running Spark version 3.3.1
[2023-05-30 15:12:48,938] {spark_submit.py:495} INFO - 23/05/30 15:12:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-05-30 15:12:49,103] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO ResourceUtils: ==============================================================
[2023-05-30 15:12:49,103] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-05-30 15:12:49,104] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO ResourceUtils: ==============================================================
[2023-05-30 15:12:49,105] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO SparkContext: Submitted application: twitter_transformation
[2023-05-30 15:12:49,134] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-05-30 15:12:49,148] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO ResourceProfile: Limiting resource is cpu
[2023-05-30 15:12:49,149] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-05-30 15:12:49,225] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO SecurityManager: Changing view acls to: raf
[2023-05-30 15:12:49,226] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO SecurityManager: Changing modify acls to: raf
[2023-05-30 15:12:49,227] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO SecurityManager: Changing view acls groups to:
[2023-05-30 15:12:49,228] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO SecurityManager: Changing modify acls groups to:
[2023-05-30 15:12:49,230] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(raf); groups with view permissions: Set(); users  with modify permissions: Set(raf); groups with modify permissions: Set()
[2023-05-30 15:12:49,642] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO Utils: Successfully started service 'sparkDriver' on port 37103.
[2023-05-30 15:12:49,708] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO SparkEnv: Registering MapOutputTracker
[2023-05-30 15:12:49,769] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO SparkEnv: Registering BlockManagerMaster
[2023-05-30 15:12:49,810] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-05-30 15:12:49,810] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-05-30 15:12:49,822] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-05-30 15:12:49,920] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-78a9033a-9d20-4879-9976-580853d35648
[2023-05-30 15:12:49,959] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2023-05-30 15:12:49,986] {spark_submit.py:495} INFO - 23/05/30 15:12:49 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-05-30 15:12:50,326] {spark_submit.py:495} INFO - 23/05/30 15:12:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-05-30 15:12:50,520] {spark_submit.py:495} INFO - 23/05/30 15:12:50 INFO Executor: Starting executor ID driver on host 172.19.51.55
[2023-05-30 15:12:50,536] {spark_submit.py:495} INFO - 23/05/30 15:12:50 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-05-30 15:12:50,579] {spark_submit.py:495} INFO - 23/05/30 15:12:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32973.
[2023-05-30 15:12:50,579] {spark_submit.py:495} INFO - 23/05/30 15:12:50 INFO NettyBlockTransferService: Server created on 172.19.51.55:32973
[2023-05-30 15:12:50,583] {spark_submit.py:495} INFO - 23/05/30 15:12:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-05-30 15:12:50,591] {spark_submit.py:495} INFO - 23/05/30 15:12:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.19.51.55, 32973, None)
[2023-05-30 15:12:50,596] {spark_submit.py:495} INFO - 23/05/30 15:12:50 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.51.55:32973 with 366.3 MiB RAM, BlockManagerId(driver, 172.19.51.55, 32973, None)
[2023-05-30 15:12:50,602] {spark_submit.py:495} INFO - 23/05/30 15:12:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.19.51.55, 32973, None)
[2023-05-30 15:12:50,606] {spark_submit.py:495} INFO - 23/05/30 15:12:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.19.51.55, 32973, None)
[2023-05-30 15:12:51,720] {spark_submit.py:495} INFO - 23/05/30 15:12:51 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-05-30 15:12:51,728] {spark_submit.py:495} INFO - 23/05/30 15:12:51 INFO SharedState: Warehouse path is 'file:/home/raf/pipeline-transform/airflow_pipeline/spark-warehouse'.
[2023-05-30 15:12:52,929] {spark_submit.py:495} INFO - 23/05/30 15:12:52 INFO InMemoryFileIndex: It took 50 ms to list leaf files for 1 paths.
[2023-05-30 15:12:53,120] {spark_submit.py:495} INFO - 23/05/30 15:12:53 INFO InMemoryFileIndex: It took 12 ms to list leaf files for 6 paths.
[2023-05-30 15:12:55,937] {spark_submit.py:495} INFO - 23/05/30 15:12:55 INFO FileSourceStrategy: Pushed Filters:
[2023-05-30 15:12:55,939] {spark_submit.py:495} INFO - 23/05/30 15:12:55 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-30 15:12:55,943] {spark_submit.py:495} INFO - 23/05/30 15:12:55 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-05-30 15:12:56,327] {spark_submit.py:495} INFO - 23/05/30 15:12:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 349.6 KiB, free 366.0 MiB)
[2023-05-30 15:12:56,389] {spark_submit.py:495} INFO - 23/05/30 15:12:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 365.9 MiB)
[2023-05-30 15:12:56,392] {spark_submit.py:495} INFO - 23/05/30 15:12:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.51.55:32973 (size: 33.9 KiB, free: 366.3 MiB)
[2023-05-30 15:12:56,398] {spark_submit.py:495} INFO - 23/05/30 15:12:56 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:12:56,524] {spark_submit.py:495} INFO - 23/05/30 15:12:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25192964 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:12:56,844] {spark_submit.py:495} INFO - 23/05/30 15:12:56 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:12:56,924] {spark_submit.py:495} INFO - 23/05/30 15:12:56 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:12:56,928] {spark_submit.py:495} INFO - 23/05/30 15:12:56 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:12:56,930] {spark_submit.py:495} INFO - 23/05/30 15:12:56 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:12:56,932] {spark_submit.py:495} INFO - 23/05/30 15:12:56 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:12:56,942] {spark_submit.py:495} INFO - 23/05/30 15:12:56 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:12:57,175] {spark_submit.py:495} INFO - 23/05/30 15:12:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 365.9 MiB)
[2023-05-30 15:12:57,182] {spark_submit.py:495} INFO - 23/05/30 15:12:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 365.9 MiB)
[2023-05-30 15:12:57,185] {spark_submit.py:495} INFO - 23/05/30 15:12:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.51.55:32973 (size: 7.0 KiB, free: 366.3 MiB)
[2023-05-30 15:12:57,188] {spark_submit.py:495} INFO - 23/05/30 15:12:57 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:12:57,212] {spark_submit.py:495} INFO - 23/05/30 15:12:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:12:57,213] {spark_submit.py:495} INFO - 23/05/30 15:12:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-05-30 15:12:57,325] {spark_submit.py:495} INFO - 23/05/30 15:12:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 5929 bytes) taskResourceAssignments Map()
[2023-05-30 15:12:57,363] {spark_submit.py:495} INFO - 23/05/30 15:12:57 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-05-30 15:12:57,816] {spark_submit.py:495} INFO - 23/05/30 15:12:57 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-28/datascience_20230528.json, range: 0-4545, partition values: [empty row]
[2023-05-30 15:12:58,262] {spark_submit.py:495} INFO - 23/05/30 15:12:58 INFO CodeGenerator: Code generated in 339.5696 ms
[2023-05-30 15:12:58,358] {spark_submit.py:495} INFO - 23/05/30 15:12:58 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-29/datascience_20230529.json, range: 0-4545, partition values: [empty row]
[2023-05-30 15:12:58,374] {spark_submit.py:495} INFO - 23/05/30 15:12:58 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-27/datascience_20230527.json, range: 0-4542, partition values: [empty row]
[2023-05-30 15:12:58,378] {spark_submit.py:495} INFO - 23/05/30 15:12:58 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-24/datascience_20230524.json, range: 0-4518, partition values: [empty row]
[2023-05-30 15:12:58,387] {spark_submit.py:495} INFO - 23/05/30 15:12:58 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-26/datascience_20230526.json, range: 0-4506, partition values: [empty row]
[2023-05-30 15:12:58,395] {spark_submit.py:495} INFO - 23/05/30 15:12:58 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-4484, partition values: [empty row]
[2023-05-30 15:12:58,426] {spark_submit.py:495} INFO - 23/05/30 15:12:58 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
[2023-05-30 15:12:58,441] {spark_submit.py:495} INFO - 23/05/30 15:12:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1138 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:12:58,444] {spark_submit.py:495} INFO - 23/05/30 15:12:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-05-30 15:12:58,453] {spark_submit.py:495} INFO - 23/05/30 15:12:58 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.474 s
[2023-05-30 15:12:58,460] {spark_submit.py:495} INFO - 23/05/30 15:12:58 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:12:58,461] {spark_submit.py:495} INFO - 23/05/30 15:12:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-05-30 15:12:58,465] {spark_submit.py:495} INFO - 23/05/30 15:12:58 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.619330 s
[2023-05-30 15:12:59,115] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO DataSourceStrategy: Pruning directories with:
[2023-05-30 15:12:59,122] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-05-30 15:12:59,125] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-05-30 15:12:59,126] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-05-30 15:12:59,258] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:12:59,259] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:12:59,261] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:12:59,608] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO CodeGenerator: Code generated in 209.7655 ms
[2023-05-30 15:12:59,619] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 349.5 KiB, free 365.6 MiB)
[2023-05-30 15:12:59,649] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.5 MiB)
[2023-05-30 15:12:59,658] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.51.55:32973 (size: 33.8 KiB, free: 366.2 MiB)
[2023-05-30 15:12:59,659] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:12:59,675] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25192964 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:12:59,774] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:12:59,777] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:12:59,777] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:12:59,777] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:12:59,778] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:12:59,779] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:12:59,834] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.2 KiB, free 365.3 MiB)
[2023-05-30 15:12:59,840] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.2 KiB, free 365.2 MiB)
[2023-05-30 15:12:59,842] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.19.51.55:32973 (size: 83.2 KiB, free: 366.1 MiB)
[2023-05-30 15:12:59,844] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:12:59,847] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:12:59,847] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-05-30 15:12:59,855] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 6273 bytes) taskResourceAssignments Map()
[2023-05-30 15:12:59,861] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-05-30 15:12:59,965] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:12:59,967] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:12:59,970] {spark_submit.py:495} INFO - 23/05/30 15:12:59 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:13:00,054] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-28/datascience_20230528.json, range: 0-4545, partition values: [19505]
[2023-05-30 15:13:00,095] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO CodeGenerator: Code generated in 34.8234 ms
[2023-05-30 15:13:00,140] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO CodeGenerator: Code generated in 11.5433 ms
[2023-05-30 15:13:00,182] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-29/datascience_20230529.json, range: 0-4545, partition values: [19506]
[2023-05-30 15:13:00,195] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-27/datascience_20230527.json, range: 0-4542, partition values: [19504]
[2023-05-30 15:13:00,208] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-24/datascience_20230524.json, range: 0-4518, partition values: [19501]
[2023-05-30 15:13:00,223] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-26/datascience_20230526.json, range: 0-4506, partition values: [19503]
[2023-05-30 15:13:00,231] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-4484, partition values: [19502]
[2023-05-30 15:13:00,261] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileOutputCommitter: Saved output of task 'attempt_202305301512596294098527417058200_0001_m_000000_1' to file:/home/raf/pipeline-transform/airflow_pipeline/curso2/Silver/twitter_datascience/tweet/process_date=2023-05-28/_temporary/0/task_202305301512596294098527417058200_0001_m_000000
[2023-05-30 15:13:00,263] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO SparkHadoopMapRedUtil: attempt_202305301512596294098527417058200_0001_m_000000_1: Committed. Elapsed time: 4 ms.
[2023-05-30 15:13:00,285] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-05-30 15:13:00,292] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 442 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:13:00,304] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-05-30 15:13:00,306] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.525 s
[2023-05-30 15:13:00,308] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:13:00,309] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-05-30 15:13:00,311] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0.536228 s
[2023-05-30 15:13:00,347] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileFormatWriter: Start to commit write Job a5df2374-49b2-4fe9-ae19-3cc71ee70d6d.
[2023-05-30 15:13:00,395] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileFormatWriter: Write Job a5df2374-49b2-4fe9-ae19-3cc71ee70d6d committed. Elapsed time: 46 ms.
[2023-05-30 15:13:00,399] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.19.51.55:32973 in memory (size: 83.2 KiB, free: 366.2 MiB)
[2023-05-30 15:13:00,411] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileFormatWriter: Finished processing stats for write job a5df2374-49b2-4fe9-ae19-3cc71ee70d6d.
[2023-05-30 15:13:00,446] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.19.51.55:32973 in memory (size: 7.0 KiB, free: 366.2 MiB)
[2023-05-30 15:13:00,487] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO DataSourceStrategy: Pruning directories with:
[2023-05-30 15:13:00,488] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileSourceStrategy: Pushed Filters:
[2023-05-30 15:13:00,488] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-30 15:13:00,489] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-05-30 15:13:00,511] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:13:00,511] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:13:00,512] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:13:00,574] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO CodeGenerator: Code generated in 23.6036 ms
[2023-05-30 15:13:00,582] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 349.5 KiB, free 365.2 MiB)
[2023-05-30 15:13:00,597] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.2 MiB)
[2023-05-30 15:13:00,598] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.19.51.55:32973 (size: 33.8 KiB, free: 366.2 MiB)
[2023-05-30 15:13:00,600] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:13:00,602] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25192964 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:13:00,639] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:13:00,640] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:13:00,640] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:13:00,640] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:13:00,640] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:13:00,643] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:13:00,679] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.2 KiB, free 365.0 MiB)
[2023-05-30 15:13:00,683] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.5 KiB, free 364.9 MiB)
[2023-05-30 15:13:00,686] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.19.51.55:32973 (size: 76.5 KiB, free: 366.1 MiB)
[2023-05-30 15:13:00,687] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:13:00,688] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:13:00,689] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-05-30 15:13:00,691] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 6273 bytes) taskResourceAssignments Map()
[2023-05-30 15:13:00,692] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-05-30 15:13:00,720] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:13:00,720] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:13:00,720] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:13:00,758] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-28/datascience_20230528.json, range: 0-4545, partition values: [19505]
[2023-05-30 15:13:00,785] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO CodeGenerator: Code generated in 21.6888 ms
[2023-05-30 15:13:00,793] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-29/datascience_20230529.json, range: 0-4545, partition values: [19506]
[2023-05-30 15:13:00,798] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-27/datascience_20230527.json, range: 0-4542, partition values: [19504]
[2023-05-30 15:13:00,804] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-24/datascience_20230524.json, range: 0-4518, partition values: [19501]
[2023-05-30 15:13:00,809] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-26/datascience_20230526.json, range: 0-4506, partition values: [19503]
[2023-05-30 15:13:00,814] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/airflow_pipeline/curso2/datalake/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-4484, partition values: [19502]
[2023-05-30 15:13:00,824] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileOutputCommitter: Saved output of task 'attempt_202305301513008792278930053669728_0002_m_000000_2' to file:/home/raf/pipeline-transform/airflow_pipeline/curso2/Silver/twitter_datascience/user/process_date=2023-05-28/_temporary/0/task_202305301513008792278930053669728_0002_m_000000
[2023-05-30 15:13:00,826] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO SparkHadoopMapRedUtil: attempt_202305301513008792278930053669728_0002_m_000000_2: Committed. Elapsed time: 3 ms.
[2023-05-30 15:13:00,829] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-05-30 15:13:00,832] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 141 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:13:00,832] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-05-30 15:13:00,836] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.189 s
[2023-05-30 15:13:00,836] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:13:00,836] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-05-30 15:13:00,837] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.197465 s
[2023-05-30 15:13:00,838] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileFormatWriter: Start to commit write Job e096126a-ea2d-4f68-8411-f60b83e49d5c.
[2023-05-30 15:13:00,860] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileFormatWriter: Write Job e096126a-ea2d-4f68-8411-f60b83e49d5c committed. Elapsed time: 21 ms.
[2023-05-30 15:13:00,862] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO FileFormatWriter: Finished processing stats for write job e096126a-ea2d-4f68-8411-f60b83e49d5c.
[2023-05-30 15:13:00,892] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO SparkContext: Invoking stop() from shutdown hook
[2023-05-30 15:13:00,906] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO SparkUI: Stopped Spark web UI at http://172.19.51.55:4040
[2023-05-30 15:13:00,927] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-05-30 15:13:00,940] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO MemoryStore: MemoryStore cleared
[2023-05-30 15:13:00,940] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO BlockManager: BlockManager stopped
[2023-05-30 15:13:00,944] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-05-30 15:13:00,947] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-05-30 15:13:00,951] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO SparkContext: Successfully stopped SparkContext
[2023-05-30 15:13:00,953] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO ShutdownHookManager: Shutdown hook called
[2023-05-30 15:13:00,954] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-f9f45554-82fd-4601-83b9-d7f807674498
[2023-05-30 15:13:00,957] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-f9f45554-82fd-4601-83b9-d7f807674498/pyspark-a1bbd583-453f-4cff-af8b-b86e51290207
[2023-05-30 15:13:00,959] {spark_submit.py:495} INFO - 23/05/30 15:13:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-c697447a-57a9-4184-b500-a00146fd43a1
[2023-05-30 15:13:01,010] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230528T000000, start_date=20230530T181244, end_date=20230530T181301
[2023-05-30 15:13:01,051] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-30 15:13:01,061] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-30 15:24:28,333] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-28T00:00:00+00:00 [queued]>
[2023-05-30 15:24:28,342] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-28T00:00:00+00:00 [queued]>
[2023-05-30 15:24:28,343] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:24:28,343] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-30 15:24:28,343] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:24:28,363] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-05-28 00:00:00+00:00
[2023-05-30 15:24:28,365] {standard_task_runner.py:52} INFO - Started process 22215 to run task
[2023-05-30 15:24:28,367] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-05-28T00:00:00+00:00', '--job-id', '46', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmph38zd3ik', '--error-file', '/tmp/tmpl_3lxwsj']
[2023-05-30 15:24:28,368] {standard_task_runner.py:80} INFO - Job 46: Subtask transform_twitter_datascience
[2023-05-30 15:24:28,419] {task_command.py:370} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-28T00:00:00+00:00 [running]> on host BIRIBA.localdomain
[2023-05-30 15:24:28,496] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=TwitterDAG
AIRFLOW_CTX_TASK_ID=transform_twitter_datascience
AIRFLOW_CTX_EXECUTION_DATE=2023-05-28T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-05-28T00:00:00+00:00
[2023-05-30 15:24:28,501] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2023-05-30 15:24:28,502] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/raf/pipeline-transform/src/spark/transformation.py --src /home/raf/pipeline-transform/datalake/bronze/twitter_datascience --dest /home/raf/pipeline-transform/datalake/silver/twitter_datascience --process-date 2023-05-28
[2023-05-30 15:24:30,417] {spark_submit.py:495} INFO - 23/05/30 15:24:30 WARN Utils: Your hostname, BIRIBA resolves to a loopback address: 127.0.1.1; using 172.19.51.55 instead (on interface eth0)
[2023-05-30 15:24:30,420] {spark_submit.py:495} INFO - 23/05/30 15:24:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-05-30 15:24:32,493] {spark_submit.py:495} INFO - 23/05/30 15:24:32 INFO SparkContext: Running Spark version 3.3.1
[2023-05-30 15:24:32,577] {spark_submit.py:495} INFO - 23/05/30 15:24:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-05-30 15:24:32,683] {spark_submit.py:495} INFO - 23/05/30 15:24:32 INFO ResourceUtils: ==============================================================
[2023-05-30 15:24:32,683] {spark_submit.py:495} INFO - 23/05/30 15:24:32 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-05-30 15:24:32,684] {spark_submit.py:495} INFO - 23/05/30 15:24:32 INFO ResourceUtils: ==============================================================
[2023-05-30 15:24:32,685] {spark_submit.py:495} INFO - 23/05/30 15:24:32 INFO SparkContext: Submitted application: twitter_transformation
[2023-05-30 15:24:32,717] {spark_submit.py:495} INFO - 23/05/30 15:24:32 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-05-30 15:24:32,731] {spark_submit.py:495} INFO - 23/05/30 15:24:32 INFO ResourceProfile: Limiting resource is cpu
[2023-05-30 15:24:32,732] {spark_submit.py:495} INFO - 23/05/30 15:24:32 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-05-30 15:24:32,811] {spark_submit.py:495} INFO - 23/05/30 15:24:32 INFO SecurityManager: Changing view acls to: raf
[2023-05-30 15:24:32,811] {spark_submit.py:495} INFO - 23/05/30 15:24:32 INFO SecurityManager: Changing modify acls to: raf
[2023-05-30 15:24:32,811] {spark_submit.py:495} INFO - 23/05/30 15:24:32 INFO SecurityManager: Changing view acls groups to:
[2023-05-30 15:24:32,812] {spark_submit.py:495} INFO - 23/05/30 15:24:32 INFO SecurityManager: Changing modify acls groups to:
[2023-05-30 15:24:32,813] {spark_submit.py:495} INFO - 23/05/30 15:24:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(raf); groups with view permissions: Set(); users  with modify permissions: Set(raf); groups with modify permissions: Set()
[2023-05-30 15:24:33,165] {spark_submit.py:495} INFO - 23/05/30 15:24:33 INFO Utils: Successfully started service 'sparkDriver' on port 41273.
[2023-05-30 15:24:33,205] {spark_submit.py:495} INFO - 23/05/30 15:24:33 INFO SparkEnv: Registering MapOutputTracker
[2023-05-30 15:24:33,253] {spark_submit.py:495} INFO - 23/05/30 15:24:33 INFO SparkEnv: Registering BlockManagerMaster
[2023-05-30 15:24:33,298] {spark_submit.py:495} INFO - 23/05/30 15:24:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-05-30 15:24:33,298] {spark_submit.py:495} INFO - 23/05/30 15:24:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-05-30 15:24:33,300] {spark_submit.py:495} INFO - 23/05/30 15:24:33 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-05-30 15:24:33,411] {spark_submit.py:495} INFO - 23/05/30 15:24:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ab033421-737a-4033-96de-b16c461c68d1
[2023-05-30 15:24:33,441] {spark_submit.py:495} INFO - 23/05/30 15:24:33 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2023-05-30 15:24:33,468] {spark_submit.py:495} INFO - 23/05/30 15:24:33 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-05-30 15:24:33,764] {spark_submit.py:495} INFO - 23/05/30 15:24:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-05-30 15:24:33,953] {spark_submit.py:495} INFO - 23/05/30 15:24:33 INFO Executor: Starting executor ID driver on host 172.19.51.55
[2023-05-30 15:24:33,967] {spark_submit.py:495} INFO - 23/05/30 15:24:33 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-05-30 15:24:34,000] {spark_submit.py:495} INFO - 23/05/30 15:24:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38365.
[2023-05-30 15:24:34,000] {spark_submit.py:495} INFO - 23/05/30 15:24:34 INFO NettyBlockTransferService: Server created on 172.19.51.55:38365
[2023-05-30 15:24:34,003] {spark_submit.py:495} INFO - 23/05/30 15:24:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-05-30 15:24:34,012] {spark_submit.py:495} INFO - 23/05/30 15:24:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.19.51.55, 38365, None)
[2023-05-30 15:24:34,018] {spark_submit.py:495} INFO - 23/05/30 15:24:34 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.51.55:38365 with 366.3 MiB RAM, BlockManagerId(driver, 172.19.51.55, 38365, None)
[2023-05-30 15:24:34,023] {spark_submit.py:495} INFO - 23/05/30 15:24:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.19.51.55, 38365, None)
[2023-05-30 15:24:34,025] {spark_submit.py:495} INFO - 23/05/30 15:24:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.19.51.55, 38365, None)
[2023-05-30 15:24:34,693] {spark_submit.py:495} INFO - 23/05/30 15:24:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-05-30 15:24:34,703] {spark_submit.py:495} INFO - 23/05/30 15:24:34 INFO SharedState: Warehouse path is 'file:/home/raf/pipeline-transform/airflow_pipeline/spark-warehouse'.
[2023-05-30 15:24:35,912] {spark_submit.py:495} INFO - 23/05/30 15:24:35 INFO InMemoryFileIndex: It took 56 ms to list leaf files for 1 paths.
[2023-05-30 15:24:36,059] {spark_submit.py:495} INFO - 23/05/30 15:24:36 INFO InMemoryFileIndex: It took 9 ms to list leaf files for 6 paths.
[2023-05-30 15:24:38,770] {spark_submit.py:495} INFO - 23/05/30 15:24:38 INFO FileSourceStrategy: Pushed Filters:
[2023-05-30 15:24:38,771] {spark_submit.py:495} INFO - 23/05/30 15:24:38 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-30 15:24:38,775] {spark_submit.py:495} INFO - 23/05/30 15:24:38 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-05-30 15:24:39,134] {spark_submit.py:495} INFO - 23/05/30 15:24:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 349.6 KiB, free 366.0 MiB)
[2023-05-30 15:24:39,200] {spark_submit.py:495} INFO - 23/05/30 15:24:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 365.9 MiB)
[2023-05-30 15:24:39,203] {spark_submit.py:495} INFO - 23/05/30 15:24:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.51.55:38365 (size: 33.9 KiB, free: 366.3 MiB)
[2023-05-30 15:24:39,209] {spark_submit.py:495} INFO - 23/05/30 15:24:39 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:24:39,347] {spark_submit.py:495} INFO - 23/05/30 15:24:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25242997 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:24:39,599] {spark_submit.py:495} INFO - 23/05/30 15:24:39 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:24:39,632] {spark_submit.py:495} INFO - 23/05/30 15:24:39 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:24:39,632] {spark_submit.py:495} INFO - 23/05/30 15:24:39 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:24:39,633] {spark_submit.py:495} INFO - 23/05/30 15:24:39 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:24:39,636] {spark_submit.py:495} INFO - 23/05/30 15:24:39 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:24:39,642] {spark_submit.py:495} INFO - 23/05/30 15:24:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:24:39,783] {spark_submit.py:495} INFO - 23/05/30 15:24:39 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 365.9 MiB)
[2023-05-30 15:24:39,788] {spark_submit.py:495} INFO - 23/05/30 15:24:39 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 365.9 MiB)
[2023-05-30 15:24:39,789] {spark_submit.py:495} INFO - 23/05/30 15:24:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.51.55:38365 (size: 7.0 KiB, free: 366.3 MiB)
[2023-05-30 15:24:39,790] {spark_submit.py:495} INFO - 23/05/30 15:24:39 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:24:39,812] {spark_submit.py:495} INFO - 23/05/30 15:24:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:24:39,814] {spark_submit.py:495} INFO - 23/05/30 15:24:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-05-30 15:24:39,881] {spark_submit.py:495} INFO - 23/05/30 15:24:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 5827 bytes) taskResourceAssignments Map()
[2023-05-30 15:24:39,900] {spark_submit.py:495} INFO - 23/05/30 15:24:39 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-05-30 15:24:40,289] {spark_submit.py:495} INFO - 23/05/30 15:24:40 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-26/datascience_20230526.json, range: 0-23089, partition values: [empty row]
[2023-05-30 15:24:40,568] {spark_submit.py:495} INFO - 23/05/30 15:24:40 INFO CodeGenerator: Code generated in 215.0895 ms
[2023-05-30 15:24:40,644] {spark_submit.py:495} INFO - 23/05/30 15:24:40 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-27/datascience_20230527.json, range: 0-18079, partition values: [empty row]
[2023-05-30 15:24:40,655] {spark_submit.py:495} INFO - 23/05/30 15:24:40 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-29/datascience_20230529.json, range: 0-13646, partition values: [empty row]
[2023-05-30 15:24:40,661] {spark_submit.py:495} INFO - 23/05/30 15:24:40 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-24/datascience_20230524.json, range: 0-9003, partition values: [empty row]
[2023-05-30 15:24:40,667] {spark_submit.py:495} INFO - 23/05/30 15:24:40 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-8881, partition values: [empty row]
[2023-05-30 15:24:40,672] {spark_submit.py:495} INFO - 23/05/30 15:24:40 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-28/datascience_20230528.json, range: 0-4475, partition values: [empty row]
[2023-05-30 15:24:40,706] {spark_submit.py:495} INFO - 23/05/30 15:24:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2806 bytes result sent to driver
[2023-05-30 15:24:40,722] {spark_submit.py:495} INFO - 23/05/30 15:24:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 856 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:24:40,724] {spark_submit.py:495} INFO - 23/05/30 15:24:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-05-30 15:24:40,750] {spark_submit.py:495} INFO - 23/05/30 15:24:40 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.084 s
[2023-05-30 15:24:40,759] {spark_submit.py:495} INFO - 23/05/30 15:24:40 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:24:40,760] {spark_submit.py:495} INFO - 23/05/30 15:24:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-05-30 15:24:40,764] {spark_submit.py:495} INFO - 23/05/30 15:24:40 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.163422 s
[2023-05-30 15:24:41,371] {spark_submit.py:495} INFO - 23/05/30 15:24:41 INFO DataSourceStrategy: Pruning directories with:
[2023-05-30 15:24:41,377] {spark_submit.py:495} INFO - 23/05/30 15:24:41 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-05-30 15:24:41,381] {spark_submit.py:495} INFO - 23/05/30 15:24:41 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-05-30 15:24:41,382] {spark_submit.py:495} INFO - 23/05/30 15:24:41 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-05-30 15:24:41,523] {spark_submit.py:495} INFO - 23/05/30 15:24:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:24:41,524] {spark_submit.py:495} INFO - 23/05/30 15:24:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:24:41,525] {spark_submit.py:495} INFO - 23/05/30 15:24:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:24:41,837] {spark_submit.py:495} INFO - 23/05/30 15:24:41 INFO CodeGenerator: Code generated in 178.4718 ms
[2023-05-30 15:24:41,844] {spark_submit.py:495} INFO - 23/05/30 15:24:41 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 349.5 KiB, free 365.6 MiB)
[2023-05-30 15:24:41,861] {spark_submit.py:495} INFO - 23/05/30 15:24:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.5 MiB)
[2023-05-30 15:24:41,865] {spark_submit.py:495} INFO - 23/05/30 15:24:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.51.55:38365 (size: 33.8 KiB, free: 366.2 MiB)
[2023-05-30 15:24:41,866] {spark_submit.py:495} INFO - 23/05/30 15:24:41 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:24:41,872] {spark_submit.py:495} INFO - 23/05/30 15:24:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25242997 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:24:41,984] {spark_submit.py:495} INFO - 23/05/30 15:24:41 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:24:41,987] {spark_submit.py:495} INFO - 23/05/30 15:24:41 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:24:41,987] {spark_submit.py:495} INFO - 23/05/30 15:24:41 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:24:41,988] {spark_submit.py:495} INFO - 23/05/30 15:24:41 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:24:41,988] {spark_submit.py:495} INFO - 23/05/30 15:24:41 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:24:41,989] {spark_submit.py:495} INFO - 23/05/30 15:24:41 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:24:42,038] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 239.1 KiB, free 365.3 MiB)
[2023-05-30 15:24:42,081] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 83.3 KiB, free 365.2 MiB)
[2023-05-30 15:24:42,083] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.19.51.55:38365 (size: 83.3 KiB, free: 366.1 MiB)
[2023-05-30 15:24:42,084] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:24:42,086] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:24:42,088] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-05-30 15:24:42,101] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 6171 bytes) taskResourceAssignments Map()
[2023-05-30 15:24:42,102] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-05-30 15:24:42,144] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.19.51.55:38365 in memory (size: 7.0 KiB, free: 366.2 MiB)
[2023-05-30 15:24:42,190] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:24:42,191] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:24:42,192] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:24:42,252] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-26/datascience_20230526.json, range: 0-23089, partition values: [19503]
[2023-05-30 15:24:42,293] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO CodeGenerator: Code generated in 34.5203 ms
[2023-05-30 15:24:42,337] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO CodeGenerator: Code generated in 10.3299 ms
[2023-05-30 15:24:42,399] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-27/datascience_20230527.json, range: 0-18079, partition values: [19504]
[2023-05-30 15:24:42,425] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-29/datascience_20230529.json, range: 0-13646, partition values: [19506]
[2023-05-30 15:24:42,440] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-24/datascience_20230524.json, range: 0-9003, partition values: [19501]
[2023-05-30 15:24:42,450] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-8881, partition values: [19502]
[2023-05-30 15:24:42,459] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-28/datascience_20230528.json, range: 0-4475, partition values: [19505]
[2023-05-30 15:24:42,497] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileOutputCommitter: Saved output of task 'attempt_202305301524418601638283952019266_0001_m_000000_1' to file:/home/raf/pipeline-transform/datalake/silver/twitter_datascience/tweet/process_date=2023-05-28/_temporary/0/task_202305301524418601638283952019266_0001_m_000000
[2023-05-30 15:24:42,499] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO SparkHadoopMapRedUtil: attempt_202305301524418601638283952019266_0001_m_000000_1: Committed. Elapsed time: 6 ms.
[2023-05-30 15:24:42,516] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-05-30 15:24:42,521] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 426 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:24:42,522] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.531 s
[2023-05-30 15:24:42,522] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:24:42,523] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-05-30 15:24:42,523] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-05-30 15:24:42,524] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0.539037 s
[2023-05-30 15:24:42,529] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileFormatWriter: Start to commit write Job f4271436-e81b-439b-8b4d-435b461f3006.
[2023-05-30 15:24:42,556] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileFormatWriter: Write Job f4271436-e81b-439b-8b4d-435b461f3006 committed. Elapsed time: 24 ms.
[2023-05-30 15:24:42,560] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileFormatWriter: Finished processing stats for write job f4271436-e81b-439b-8b4d-435b461f3006.
[2023-05-30 15:24:42,624] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO DataSourceStrategy: Pruning directories with:
[2023-05-30 15:24:42,626] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileSourceStrategy: Pushed Filters:
[2023-05-30 15:24:42,626] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-30 15:24:42,627] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-05-30 15:24:42,643] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:24:42,644] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:24:42,646] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:24:42,723] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO CodeGenerator: Code generated in 24.7532 ms
[2023-05-30 15:24:42,730] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 349.5 KiB, free 364.9 MiB)
[2023-05-30 15:24:42,744] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 364.9 MiB)
[2023-05-30 15:24:42,745] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.19.51.55:38365 (size: 33.8 KiB, free: 366.1 MiB)
[2023-05-30 15:24:42,749] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:24:42,750] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 25242997 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:24:42,784] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:24:42,786] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:24:42,786] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:24:42,786] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:24:42,788] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:24:42,790] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:24:42,821] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 216.1 KiB, free 364.7 MiB)
[2023-05-30 15:24:42,825] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.5 KiB, free 364.6 MiB)
[2023-05-30 15:24:42,827] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.19.51.55:38365 (size: 76.5 KiB, free: 366.0 MiB)
[2023-05-30 15:24:42,829] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:24:42,830] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:24:42,831] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-05-30 15:24:42,834] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 6171 bytes) taskResourceAssignments Map()
[2023-05-30 15:24:42,834] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-05-30 15:24:42,860] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:24:42,861] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:24:42,862] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:24:42,901] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-26/datascience_20230526.json, range: 0-23089, partition values: [19503]
[2023-05-30 15:24:42,924] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO CodeGenerator: Code generated in 18.9688 ms
[2023-05-30 15:24:42,938] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-27/datascience_20230527.json, range: 0-18079, partition values: [19504]
[2023-05-30 15:24:42,945] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-29/datascience_20230529.json, range: 0-13646, partition values: [19506]
[2023-05-30 15:24:42,952] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-24/datascience_20230524.json, range: 0-9003, partition values: [19501]
[2023-05-30 15:24:42,957] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-25/datascience_20230525.json, range: 0-8881, partition values: [19502]
[2023-05-30 15:24:42,963] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-28/datascience_20230528.json, range: 0-4475, partition values: [19505]
[2023-05-30 15:24:42,977] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO FileOutputCommitter: Saved output of task 'attempt_202305301524423554934231962416653_0002_m_000000_2' to file:/home/raf/pipeline-transform/datalake/silver/twitter_datascience/user/process_date=2023-05-28/_temporary/0/task_202305301524423554934231962416653_0002_m_000000
[2023-05-30 15:24:42,977] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO SparkHadoopMapRedUtil: attempt_202305301524423554934231962416653_0002_m_000000_2: Committed. Elapsed time: 2 ms.
[2023-05-30 15:24:42,980] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-05-30 15:24:42,989] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 155 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:24:42,997] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.198 s
[2023-05-30 15:24:42,997] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:24:42,998] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-05-30 15:24:42,998] {spark_submit.py:495} INFO - 23/05/30 15:24:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-05-30 15:24:43,002] {spark_submit.py:495} INFO - 23/05/30 15:24:43 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.215992 s
[2023-05-30 15:24:43,002] {spark_submit.py:495} INFO - 23/05/30 15:24:43 INFO FileFormatWriter: Start to commit write Job 905873ad-333b-4143-a155-51deb037e9eb.
[2023-05-30 15:24:43,031] {spark_submit.py:495} INFO - 23/05/30 15:24:43 INFO FileFormatWriter: Write Job 905873ad-333b-4143-a155-51deb037e9eb committed. Elapsed time: 27 ms.
[2023-05-30 15:24:43,032] {spark_submit.py:495} INFO - 23/05/30 15:24:43 INFO FileFormatWriter: Finished processing stats for write job 905873ad-333b-4143-a155-51deb037e9eb.
[2023-05-30 15:24:43,083] {spark_submit.py:495} INFO - 23/05/30 15:24:43 INFO SparkContext: Invoking stop() from shutdown hook
[2023-05-30 15:24:43,102] {spark_submit.py:495} INFO - 23/05/30 15:24:43 INFO SparkUI: Stopped Spark web UI at http://172.19.51.55:4040
[2023-05-30 15:24:43,124] {spark_submit.py:495} INFO - 23/05/30 15:24:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-05-30 15:24:43,137] {spark_submit.py:495} INFO - 23/05/30 15:24:43 INFO MemoryStore: MemoryStore cleared
[2023-05-30 15:24:43,138] {spark_submit.py:495} INFO - 23/05/30 15:24:43 INFO BlockManager: BlockManager stopped
[2023-05-30 15:24:43,142] {spark_submit.py:495} INFO - 23/05/30 15:24:43 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-05-30 15:24:43,146] {spark_submit.py:495} INFO - 23/05/30 15:24:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-05-30 15:24:43,153] {spark_submit.py:495} INFO - 23/05/30 15:24:43 INFO SparkContext: Successfully stopped SparkContext
[2023-05-30 15:24:43,153] {spark_submit.py:495} INFO - 23/05/30 15:24:43 INFO ShutdownHookManager: Shutdown hook called
[2023-05-30 15:24:43,154] {spark_submit.py:495} INFO - 23/05/30 15:24:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-c0fd4db2-eebc-441a-a328-ef2a12fa539e/pyspark-19cf8d04-ccf6-4dff-8ed0-1d9fadaf4761
[2023-05-30 15:24:43,157] {spark_submit.py:495} INFO - 23/05/30 15:24:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-987ab2d9-1927-4eee-af1b-0c675c173c11
[2023-05-30 15:24:43,160] {spark_submit.py:495} INFO - 23/05/30 15:24:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-c0fd4db2-eebc-441a-a328-ef2a12fa539e
[2023-05-30 15:24:43,226] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230528T000000, start_date=20230530T182428, end_date=20230530T182443
[2023-05-30 15:24:43,274] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-30 15:24:43,285] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-05-30 15:56:24,812] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-28T00:00:00+00:00 [queued]>
[2023-05-30 15:56:24,819] {taskinstance.py:1159} INFO - Dependencies all met for <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-28T00:00:00+00:00 [queued]>
[2023-05-30 15:56:24,819] {taskinstance.py:1356} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:56:24,819] {taskinstance.py:1357} INFO - Starting attempt 1 of 1
[2023-05-30 15:56:24,819] {taskinstance.py:1358} INFO - 
--------------------------------------------------------------------------------
[2023-05-30 15:56:24,840] {taskinstance.py:1377} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_datascience> on 2023-05-28 00:00:00+00:00
[2023-05-30 15:56:24,842] {standard_task_runner.py:52} INFO - Started process 28406 to run task
[2023-05-30 15:56:24,845] {standard_task_runner.py:79} INFO - Running: ['airflow', 'tasks', 'run', 'TwitterDAG', 'transform_twitter_datascience', 'scheduled__2023-05-28T00:00:00+00:00', '--job-id', '50', '--raw', '--subdir', 'DAGS_FOLDER/twitter_dag.py', '--cfg-path', '/tmp/tmpmnya5cw5', '--error-file', '/tmp/tmpw5qoomji']
[2023-05-30 15:56:24,846] {standard_task_runner.py:80} INFO - Job 50: Subtask transform_twitter_datascience
[2023-05-30 15:56:24,892] {task_command.py:370} INFO - Running <TaskInstance: TwitterDAG.transform_twitter_datascience scheduled__2023-05-28T00:00:00+00:00 [running]> on host BIRIBA.localdomain
[2023-05-30 15:56:24,955] {taskinstance.py:1569} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=TwitterDAG
AIRFLOW_CTX_TASK_ID=transform_twitter_datascience
AIRFLOW_CTX_EXECUTION_DATE=2023-05-28T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-05-28T00:00:00+00:00
[2023-05-30 15:56:24,959] {base.py:68} INFO - Using connection ID 'spark_default' for task execution.
[2023-05-30 15:56:24,961] {spark_submit.py:344} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation --queue root.default /home/raf/pipeline-transform/src/spark/transformation.py --src /home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-28 --dest /home/raf/pipeline-transform/datalake/silver/twitter_datascience/extract_date=2023-05-28 --process-date 2023-05-28
[2023-05-30 15:56:26,805] {spark_submit.py:495} INFO - 23/05/30 15:56:26 WARN Utils: Your hostname, BIRIBA resolves to a loopback address: 127.0.1.1; using 172.19.51.55 instead (on interface eth0)
[2023-05-30 15:56:26,807] {spark_submit.py:495} INFO - 23/05/30 15:56:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2023-05-30 15:56:29,009] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO SparkContext: Running Spark version 3.3.1
[2023-05-30 15:56:29,119] {spark_submit.py:495} INFO - 23/05/30 15:56:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-05-30 15:56:29,295] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO ResourceUtils: ==============================================================
[2023-05-30 15:56:29,296] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO ResourceUtils: No custom resources configured for spark.driver.
[2023-05-30 15:56:29,297] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO ResourceUtils: ==============================================================
[2023-05-30 15:56:29,298] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO SparkContext: Submitted application: twitter_transformation
[2023-05-30 15:56:29,325] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2023-05-30 15:56:29,339] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO ResourceProfile: Limiting resource is cpu
[2023-05-30 15:56:29,340] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2023-05-30 15:56:29,406] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO SecurityManager: Changing view acls to: raf
[2023-05-30 15:56:29,407] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO SecurityManager: Changing modify acls to: raf
[2023-05-30 15:56:29,408] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO SecurityManager: Changing view acls groups to:
[2023-05-30 15:56:29,409] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO SecurityManager: Changing modify acls groups to:
[2023-05-30 15:56:29,410] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(raf); groups with view permissions: Set(); users  with modify permissions: Set(raf); groups with modify permissions: Set()
[2023-05-30 15:56:29,744] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO Utils: Successfully started service 'sparkDriver' on port 44999.
[2023-05-30 15:56:29,777] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO SparkEnv: Registering MapOutputTracker
[2023-05-30 15:56:29,839] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO SparkEnv: Registering BlockManagerMaster
[2023-05-30 15:56:29,873] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2023-05-30 15:56:29,874] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2023-05-30 15:56:29,890] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-05-30 15:56:29,975] {spark_submit.py:495} INFO - 23/05/30 15:56:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-294512c2-b648-41a5-9173-0a677ffe0e0d
[2023-05-30 15:56:30,011] {spark_submit.py:495} INFO - 23/05/30 15:56:30 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[2023-05-30 15:56:30,042] {spark_submit.py:495} INFO - 23/05/30 15:56:30 INFO SparkEnv: Registering OutputCommitCoordinator
[2023-05-30 15:56:30,372] {spark_submit.py:495} INFO - 23/05/30 15:56:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2023-05-30 15:56:30,538] {spark_submit.py:495} INFO - 23/05/30 15:56:30 INFO Executor: Starting executor ID driver on host 172.19.51.55
[2023-05-30 15:56:30,550] {spark_submit.py:495} INFO - 23/05/30 15:56:30 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2023-05-30 15:56:30,583] {spark_submit.py:495} INFO - 23/05/30 15:56:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42789.
[2023-05-30 15:56:30,583] {spark_submit.py:495} INFO - 23/05/30 15:56:30 INFO NettyBlockTransferService: Server created on 172.19.51.55:42789
[2023-05-30 15:56:30,586] {spark_submit.py:495} INFO - 23/05/30 15:56:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2023-05-30 15:56:30,594] {spark_submit.py:495} INFO - 23/05/30 15:56:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.19.51.55, 42789, None)
[2023-05-30 15:56:30,603] {spark_submit.py:495} INFO - 23/05/30 15:56:30 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.51.55:42789 with 366.3 MiB RAM, BlockManagerId(driver, 172.19.51.55, 42789, None)
[2023-05-30 15:56:30,607] {spark_submit.py:495} INFO - 23/05/30 15:56:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.19.51.55, 42789, None)
[2023-05-30 15:56:30,610] {spark_submit.py:495} INFO - 23/05/30 15:56:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.19.51.55, 42789, None)
[2023-05-30 15:56:31,244] {spark_submit.py:495} INFO - 23/05/30 15:56:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2023-05-30 15:56:31,253] {spark_submit.py:495} INFO - 23/05/30 15:56:31 INFO SharedState: Warehouse path is 'file:/home/raf/pipeline-transform/airflow_pipeline/spark-warehouse'.
[2023-05-30 15:56:33,724] {spark_submit.py:495} INFO - 23/05/30 15:56:33 INFO InMemoryFileIndex: It took 52 ms to list leaf files for 1 paths.
[2023-05-30 15:56:33,823] {spark_submit.py:495} INFO - 23/05/30 15:56:33 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
[2023-05-30 15:56:38,820] {spark_submit.py:495} INFO - 23/05/30 15:56:38 INFO FileSourceStrategy: Pushed Filters:
[2023-05-30 15:56:38,824] {spark_submit.py:495} INFO - 23/05/30 15:56:38 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-30 15:56:38,828] {spark_submit.py:495} INFO - 23/05/30 15:56:38 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
[2023-05-30 15:56:39,550] {spark_submit.py:495} INFO - 23/05/30 15:56:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 349.6 KiB, free 366.0 MiB)
[2023-05-30 15:56:39,904] {spark_submit.py:495} INFO - 23/05/30 15:56:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 365.9 MiB)
[2023-05-30 15:56:39,908] {spark_submit.py:495} INFO - 23/05/30 15:56:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.51.55:42789 (size: 33.9 KiB, free: 366.3 MiB)
[2023-05-30 15:56:39,914] {spark_submit.py:495} INFO - 23/05/30 15:56:39 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:56:39,929] {spark_submit.py:495} INFO - 23/05/30 15:56:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198734 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:56:40,347] {spark_submit.py:495} INFO - 23/05/30 15:56:40 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:56:40,388] {spark_submit.py:495} INFO - 23/05/30 15:56:40 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:56:40,391] {spark_submit.py:495} INFO - 23/05/30 15:56:40 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:56:40,392] {spark_submit.py:495} INFO - 23/05/30 15:56:40 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:56:40,394] {spark_submit.py:495} INFO - 23/05/30 15:56:40 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:56:40,411] {spark_submit.py:495} INFO - 23/05/30 15:56:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:56:40,631] {spark_submit.py:495} INFO - 23/05/30 15:56:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KiB, free 365.9 MiB)
[2023-05-30 15:56:40,640] {spark_submit.py:495} INFO - 23/05/30 15:56:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 365.9 MiB)
[2023-05-30 15:56:40,641] {spark_submit.py:495} INFO - 23/05/30 15:56:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.51.55:42789 (size: 7.0 KiB, free: 366.3 MiB)
[2023-05-30 15:56:40,647] {spark_submit.py:495} INFO - 23/05/30 15:56:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:56:40,679] {spark_submit.py:495} INFO - 23/05/30 15:56:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:56:40,686] {spark_submit.py:495} INFO - 23/05/30 15:56:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2023-05-30 15:56:40,777] {spark_submit.py:495} INFO - 23/05/30 15:56:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 4992 bytes) taskResourceAssignments Map()
[2023-05-30 15:56:40,796] {spark_submit.py:495} INFO - 23/05/30 15:56:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2023-05-30 15:56:41,362] {spark_submit.py:495} INFO - 23/05/30 15:56:41 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-28/datascience_20230528.json, range: 0-4430, partition values: [empty row]
[2023-05-30 15:56:41,739] {spark_submit.py:495} INFO - 23/05/30 15:56:41 INFO CodeGenerator: Code generated in 293.8009 ms
[2023-05-30 15:56:41,869] {spark_submit.py:495} INFO - 23/05/30 15:56:41 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
[2023-05-30 15:56:41,904] {spark_submit.py:495} INFO - 23/05/30 15:56:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1135 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:56:41,908] {spark_submit.py:495} INFO - 23/05/30 15:56:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2023-05-30 15:56:41,924] {spark_submit.py:495} INFO - 23/05/30 15:56:41 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.471 s
[2023-05-30 15:56:41,930] {spark_submit.py:495} INFO - 23/05/30 15:56:41 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:56:41,941] {spark_submit.py:495} INFO - 23/05/30 15:56:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2023-05-30 15:56:41,956] {spark_submit.py:495} INFO - 23/05/30 15:56:41 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.598682 s
[2023-05-30 15:56:42,842] {spark_submit.py:495} INFO - 23/05/30 15:56:42 INFO FileSourceStrategy: Pushed Filters: IsNotNull(data)
[2023-05-30 15:56:42,845] {spark_submit.py:495} INFO - 23/05/30 15:56:42 INFO FileSourceStrategy: Post-Scan Filters: (size(data#8, true) > 0),isnotnull(data#8)
[2023-05-30 15:56:42,846] {spark_submit.py:495} INFO - 23/05/30 15:56:42 INFO FileSourceStrategy: Output Data Schema: struct<data: array<struct<author_id:string,conversation_id:string,created_at:string,edit_history_tweet_ids:array<bigint>,id:string,in_reply_to_user_id:string,lang:string,public_metrics:struct<like_count:bigint,quote_count:bigint,reply_count:bigint,retweet_count:bigint>,text:string>>>
[2023-05-30 15:56:42,994] {spark_submit.py:495} INFO - 23/05/30 15:56:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:56:42,994] {spark_submit.py:495} INFO - 23/05/30 15:56:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:56:42,995] {spark_submit.py:495} INFO - 23/05/30 15:56:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:56:43,519] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO CodeGenerator: Code generated in 286.7891 ms
[2023-05-30 15:56:43,531] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 349.5 KiB, free 365.6 MiB)
[2023-05-30 15:56:43,556] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 365.5 MiB)
[2023-05-30 15:56:43,559] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.51.55:42789 (size: 33.8 KiB, free: 366.2 MiB)
[2023-05-30 15:56:43,561] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO SparkContext: Created broadcast 2 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:56:43,570] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198734 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:56:43,694] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:56:43,699] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:56:43,699] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO DAGScheduler: Final stage: ResultStage 1 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:56:43,699] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:56:43,699] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:56:43,705] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO DAGScheduler: Submitting ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:56:43,746] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.19.51.55:42789 in memory (size: 7.0 KiB, free: 366.2 MiB)
[2023-05-30 15:56:43,792] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 234.6 KiB, free 365.3 MiB)
[2023-05-30 15:56:43,796] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 81.6 KiB, free 365.2 MiB)
[2023-05-30 15:56:43,798] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.19.51.55:42789 (size: 81.6 KiB, free: 366.2 MiB)
[2023-05-30 15:56:43,799] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:56:43,800] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (CoalescedRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:56:43,801] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2023-05-30 15:56:43,809] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 5221 bytes) taskResourceAssignments Map()
[2023-05-30 15:56:43,812] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2023-05-30 15:56:43,927] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:56:43,929] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:56:43,930] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:56:43,984] {spark_submit.py:495} INFO - 23/05/30 15:56:43 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-28/datascience_20230528.json, range: 0-4430, partition values: [empty row]
[2023-05-30 15:56:44,033] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO CodeGenerator: Code generated in 38.6555 ms
[2023-05-30 15:56:44,069] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO CodeGenerator: Code generated in 7.6566 ms
[2023-05-30 15:56:44,111] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO FileOutputCommitter: Saved output of task 'attempt_202305301556436426289550973956876_0001_m_000000_1' to file:/home/raf/pipeline-transform/datalake/silver/twitter_datascience/extract_date=2023-05-28/tweet/process_date=2023-05-28/_temporary/0/task_202305301556436426289550973956876_0001_m_000000
[2023-05-30 15:56:44,112] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO SparkHadoopMapRedUtil: attempt_202305301556436426289550973956876_0001_m_000000_1: Committed. Elapsed time: 1 ms.
[2023-05-30 15:56:44,121] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2678 bytes result sent to driver
[2023-05-30 15:56:44,124] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 319 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:56:44,124] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2023-05-30 15:56:44,126] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO DAGScheduler: ResultStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.421 s
[2023-05-30 15:56:44,127] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:56:44,127] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2023-05-30 15:56:44,128] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 0.432488 s
[2023-05-30 15:56:44,130] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO FileFormatWriter: Start to commit write Job 9420dd0f-206e-40a2-a1f8-bd0a696595aa.
[2023-05-30 15:56:44,151] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO FileFormatWriter: Write Job 9420dd0f-206e-40a2-a1f8-bd0a696595aa committed. Elapsed time: 20 ms.
[2023-05-30 15:56:44,156] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO FileFormatWriter: Finished processing stats for write job 9420dd0f-206e-40a2-a1f8-bd0a696595aa.
[2023-05-30 15:56:44,213] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO FileSourceStrategy: Pushed Filters:
[2023-05-30 15:56:44,213] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO FileSourceStrategy: Post-Scan Filters:
[2023-05-30 15:56:44,214] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO FileSourceStrategy: Output Data Schema: struct<includes: struct<users: array<struct<created_at:string,id:string,name:string,username:string>>>>
[2023-05-30 15:56:44,226] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:56:44,226] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:56:44,226] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:56:44,267] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO CodeGenerator: Code generated in 15.8335 ms
[2023-05-30 15:56:44,275] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 349.5 KiB, free 364.9 MiB)
[2023-05-30 15:56:44,320] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 33.8 KiB, free 364.9 MiB)
[2023-05-30 15:56:44,321] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.19.51.55:42789 (size: 33.8 KiB, free: 366.1 MiB)
[2023-05-30 15:56:44,332] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO SparkContext: Created broadcast 4 from json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:56:44,335] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4198734 bytes, open cost is considered as scanning 4194304 bytes.
[2023-05-30 15:56:44,372] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[2023-05-30 15:56:44,375] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO DAGScheduler: Got job 2 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2023-05-30 15:56:44,378] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)
[2023-05-30 15:56:44,378] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO DAGScheduler: Parents of final stage: List()
[2023-05-30 15:56:44,378] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO DAGScheduler: Missing parents: List()
[2023-05-30 15:56:44,378] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
[2023-05-30 15:56:44,431] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 214.7 KiB, free 364.7 MiB)
[2023-05-30 15:56:44,441] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 76.3 KiB, free 364.6 MiB)
[2023-05-30 15:56:44,445] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.19.51.55:42789 (size: 76.3 KiB, free: 366.0 MiB)
[2023-05-30 15:56:44,448] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
[2023-05-30 15:56:44,454] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2023-05-30 15:56:44,455] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2023-05-30 15:56:44,459] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.19.51.55, executor driver, partition 0, PROCESS_LOCAL, 5221 bytes) taskResourceAssignments Map()
[2023-05-30 15:56:44,463] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2023-05-30 15:56:44,527] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2023-05-30 15:56:44,528] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2023-05-30 15:56:44,531] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
[2023-05-30 15:56:44,587] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO FileScanRDD: Reading File path: file:///home/raf/pipeline-transform/datalake/bronze/twitter_datascience/extract_date=2023-05-28/datascience_20230528.json, range: 0-4430, partition values: [empty row]
[2023-05-30 15:56:44,633] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO CodeGenerator: Code generated in 38.0155 ms
[2023-05-30 15:56:44,664] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO FileOutputCommitter: Saved output of task 'attempt_202305301556441113098009503424181_0002_m_000000_2' to file:/home/raf/pipeline-transform/datalake/silver/twitter_datascience/extract_date=2023-05-28/user/process_date=2023-05-28/_temporary/0/task_202305301556441113098009503424181_0002_m_000000
[2023-05-30 15:56:44,665] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO SparkHadoopMapRedUtil: attempt_202305301556441113098009503424181_0002_m_000000_2: Committed. Elapsed time: 11 ms.
[2023-05-30 15:56:44,667] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2579 bytes result sent to driver
[2023-05-30 15:56:44,678] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 220 ms on 172.19.51.55 (executor driver) (1/1)
[2023-05-30 15:56:44,679] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2023-05-30 15:56:44,681] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 0.298 s
[2023-05-30 15:56:44,681] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2023-05-30 15:56:44,681] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2023-05-30 15:56:44,682] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO DAGScheduler: Job 2 finished: json at NativeMethodAccessorImpl.java:0, took 0.309167 s
[2023-05-30 15:56:44,686] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO FileFormatWriter: Start to commit write Job 592ebfad-c845-4c11-af73-6fd8a4cd4889.
[2023-05-30 15:56:44,740] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO FileFormatWriter: Write Job 592ebfad-c845-4c11-af73-6fd8a4cd4889 committed. Elapsed time: 52 ms.
[2023-05-30 15:56:44,741] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO FileFormatWriter: Finished processing stats for write job 592ebfad-c845-4c11-af73-6fd8a4cd4889.
[2023-05-30 15:56:44,802] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO SparkContext: Invoking stop() from shutdown hook
[2023-05-30 15:56:44,827] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO SparkUI: Stopped Spark web UI at http://172.19.51.55:4040
[2023-05-30 15:56:44,872] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2023-05-30 15:56:44,890] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO MemoryStore: MemoryStore cleared
[2023-05-30 15:56:44,891] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO BlockManager: BlockManager stopped
[2023-05-30 15:56:44,896] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO BlockManagerMaster: BlockManagerMaster stopped
[2023-05-30 15:56:44,901] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2023-05-30 15:56:44,907] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO SparkContext: Successfully stopped SparkContext
[2023-05-30 15:56:44,908] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO ShutdownHookManager: Shutdown hook called
[2023-05-30 15:56:44,910] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-e7c87f39-06bb-450e-88a9-19e936971c1b
[2023-05-30 15:56:44,923] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-969bf1b5-d940-4153-b24d-f63927e20bd7/pyspark-c2ab18b8-0c1e-4a04-bb41-ef56fdbdd296
[2023-05-30 15:56:44,932] {spark_submit.py:495} INFO - 23/05/30 15:56:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-969bf1b5-d940-4153-b24d-f63927e20bd7
[2023-05-30 15:56:45,019] {taskinstance.py:1395} INFO - Marking task as SUCCESS. dag_id=TwitterDAG, task_id=transform_twitter_datascience, execution_date=20230528T000000, start_date=20230530T185624, end_date=20230530T185645
[2023-05-30 15:56:45,091] {local_task_job.py:156} INFO - Task exited with return code 0
[2023-05-30 15:56:45,143] {local_task_job.py:273} INFO - 1 downstream tasks scheduled from follow-on schedule check
